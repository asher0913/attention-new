# Attention-CEM 项目组会汇报

## 摘要
- **目标**：替换 CEM-main 中基于 GMM 的条件熵代理，构建端到端可学习的 Slot Attention + Cross Attention 架构，并在此基础上引入门控等数值稳定手段。
- **成果**：完成注意力模块的实现、训练流程接入、历史返回值 bug 修复；但当前注意力模块未受控训练导致分类准确率偏低、防御效果极强。
- **待解决问题**：注意力参数规模、优化器管理、阈值策略、环境复现性，需要在会后继续迭代。

---

## 1. 原始 GMM 条件熵代理回顾
### 1.1 核心位置
- `CEM-main/model_training_paral_pruning.py:885` 的 `compute_class_means`：对每个类别做 KMeans/GMM 聚类，计算类内方差并加权求得条件熵 surrogate。
- `CEM-main/model_training_paral_pruning.py:1046`：在客户端训练步骤中调用 GMM 版本的 `compute_class_means`，并结合噪声/防御策略更新梯度。
- 聚类前处理（PCA/GMM 拟合）位于 `CEM-main/model_training_paral_pruning.py:626` 之后的一系列工具函数，依赖 scikit-learn CPU 端实现，训练过程中频繁在 GPU/CPU 之间切换。

### 1.2 原始处理流程
1. 将 `self.f(x_private)` 的特征按照类别拆分并扁平化。
2. 针对每个类别，调用预先保存的 `centroids_list / weights_list / cluster_variances_list` 或直接运行 GMM；获取簇权重与方差。
3. 计算每个簇的加权方差，再聚合成类内条件熵 surrogate（参见 `CEM-main/model_training_paral_pruning.py:943-1005`）。
4. 在 `train_target_step` 中把该 surrogate 作为鲁棒正则，与交叉熵共同回传梯度。

### 1.3 原始方案的瓶颈
- GMM 通常需要 CPU 拟合，特征维度高时需要 PCA 降维，计算耗时且实现复杂。
- 对噪声敏感，需要手动处理协方差正定性、Cholesky 分解失败等问题。
- 条件熵 surrogate 不可学习，超参（簇数、PCA 维数、阈值）难以在不同任务之间迁移。

---

## 2. 注意力架构替换方案
### 2.1 新增模块概览
- `SlotAttention`（`model_training_paral_pruning.py:40`）：受 Locatello 等工作启发，对输入 token 进行多轮 slot 聚合。
- `CrossAttention`（`model_training_paral_pruning.py:94`）：参考 Flamingo 的 Gated XAttn-Dense 结构，实现 slot 到 query 的跨注意力，并引入门控与 FFN。
- `SlotCrossAttentionCEM`（`model_training_paral_pruning.py:138`）：组合上述两者，输出每个类别的类内 MSE 与 “条件熵” surrogate。

### 2.2 接入训练流程
1. 在 `MIA_train.__init__` 中强制开启注意力 CEM，关闭旧 GMM 流程（`model_training_paral_pruning.py:504-506`）。
2. `train_target_step`（`model_training_paral_pruning.py:1224-1255`）中：
   - 对客户端激活 `z_private` 进行扁平化（若为 4D 则 `view(B, -1)`）。
   - 首次运行时延迟初始化 `self.attention_cem`，设置 slot 数、head 数、迭代次数等超参。
   - 执行前向计算，获得 `rob_loss`（条件熵 surrogate）与 `intra_class_mse`（监控指标）。
   - 对数值异常（NaN/Inf）进行兜底处理，防止训练崩溃。
3. 计算出的 `rob_loss` 通过手动梯度拼接的方式作用于客户端编码器 `self.f`，保持与原始 GMM 相同的梯度注入逻辑（`model_training_paral_pruning.py:1387-1417`）。
4. 同步在集中式训练脚本 `model_training.py:953-1001`、`model_training.py:1642-1652` 接入注意力 surrogate，以支持非切分场景。

### 2.3 直接影响
- **效率**：避免了 CPU 端 GMM 拟合与 PCA 预处理，所有操作在 GPU 张量上完成。
- **可学习性**：Surrogate 从固定统计量升级为可学习的注意力模块，理论上能自适应不同任务。
- **可扩展性**：Slot 数、head 数等超参可根据数据规模调整，有望统一多数据集配置。
- **稳定性**：通过 LayerNorm、方差裁剪等操作减少 NaN/Inf，但也引入新的梯度管理需求。

---

## 3. 基于注意力的门控与稳定性增强
### 3.1 Flamingo 风格门控
- 在 `CrossAttention` 中引入门控残差：`y = query + tanh(alpha_xattn) * out`；FFN 同理（`model_training_paral_pruning.py:108-135`）。
- 参数 `alpha_xattn`、`alpha_ffn` 可学习，初始化为 0.1，确保初期保留原始残差，逐步放大注意力贡献。
- 效果：可以按需调节跨注意力与 FFN 的影响，减少梯度震荡；避免一开始完全依赖注意力导致训练发散。

### 3.2 数值稳定措施
- SlotAttention 内部对注意力 logits 进行温度缩放并添加 `eps` 防止除零（`model_training_paral_pruning.py:77-85`）。
- `SlotCrossAttentionCEM` 对每个类别的特征进行 LayerNorm，并对方差执行 `torch.clamp` 与 `log` 监督，同时设置阈值 `logvar_thr`（`model_training_paral_pruning.py:169-189`）。
- 训练环节对 NaN/Inf 进行兜底，避免向后传播异常梯度（`model_training_paral_pruning.py:1249-1255`）。

### 3.3 辅助修复
- `train_target_step` 返回值修复（`model_training_paral_pruning.py:1422`）确保训练曲线正常。
- `FIXES_SUMMARY.md` 记录了 Prec@1=10%、t-SNE 崩溃等历史问题的解决过程，便于回顾。

---

## 4. 整体影响评估
### 4.1 优势
- **计算端**：彻底移除 GMM 拟合与 PCA 需求，节省 CPU/GPU 之间的数据搬运，脚本运行更加顺畅。
- **统一性**：slot attention 对不同特征形状可统一处理，减少为每个数据集单独调整 GMM 超参的需求。
- **稳定性改进**：依靠门控与归一化手段，训练过程中较少出现历史上的段错误。
- **防御强化**：`rob_loss` 对类内方差的压制更强烈，在实测中表现为“防御特别好”。

### 4.2 劣势
- **分类准确率显著下降**：注意力模块参数极大（直接对 `B×(C×H×W)` 的扁平向量做 slot attention），梯度噪声大，导致主任务学习困难。
- **优化器缺失**：`attention_cem` 并未加入 `optimizer` 或执行 `zero_grad()`，其参数基本停留在初始化状态，无法发挥“可学习”优势。
- **阈值耦合问题**：`logvar_thr` 与 `regularization_strength**2` 关联，当正则强度为 0 附近时，阈值约为 1e-6，等价于强制压缩特征方差。
- **环境复现性**：`requirements.txt` 不含版本号，缺乏统一环境会加剧数值差异。

---

## 5. 当前挑战与问题域
1. **注意力参数规模**  
   - 直接扁平化特征后参数量上亿级别，需考虑增加降维头或 token 化处理。
2. **注意力模块的训练策略**  
   - 需决定是否单独优化、共享优化器或冻结；否则会持续处于随机状态影响主任务。
3. **阈值与损失设计**  
   - 需要重新评估 `var_threshold` 与 `regularization_strength` 的联动关系，避免过度惩罚。
4. **评估基准缺失**  
   - 应保留 GMM 路径，至少做一次对照实验，验证注意力替换的收益/代价。
5. **环境与复现**  
   - 建议补充带版本的依赖配置，避免团队成员运行时出现新的数值不稳定。

---

## 6. 建议的后续计划
1. **结构改造**  
   - 在进入 Slot Attention 前加入线性降维或 `[B, H×W, C]` 的 token 处理，将特征维度压缩到 128 左右。
2. **训练策略**  
   - 为 `attention_cem` 配置专门的 Adam 优化器并在 `optimizer_zero_grad` 中同步清梯度；若短期内难以调参，可先冻结其参数。
3. **阈值重设计**  
   - 参考类内方差统计，设定固定阈值或逐步增大的 schedule，确保主任务不会被过早压缩。
4. **Baseline 对照**  
   - 恢复 GMM 分支开关，重新跑一次实验，记录准确率、防御指标与时间开销，为组会上提供量化对比。
5. **实验验证**  
   - 调整后运行 `bash run_exp.sh` 或缩短版实验，输出训练/验证曲线，确定改动效果。
6. **环境管理**  
   - 形成 `requirements.txt` 的推荐版本或 Conda YAML，保证后续实验一致性。

---

## 7. 组会重点提纲
1. **替换逻辑**：说明如何从 GMM (CPU + PCA) 切换为 Slot/Cross Attention（GPU 端可学习），引用 `model_training_paral_pruning.py:40-208`、`model_training_paral_pruning.py:1224-1255`。
2. **门控与稳定性策略**：强调 Flamingo 门控、LayerNorm、方差裁剪的作用及其代码位置。
3. **实际影响**：防御显著增强但分类准确率下降的原因（参数规模、未训练、阈值设定）。
4. **挑战与计划**：阐述上述改进建议及实验安排，明确下一阶段要验证的点。

---

## 8. 附录：关键代码索引
- 注意力模块定义：`model_training_paral_pruning.py:40`、`model_training_paral_pruning.py:94`、`model_training_paral_pruning.py:138`
- 注意力替换接入：`model_training_paral_pruning.py:504-506`、`model_training_paral_pruning.py:1224-1255`
- 注意力梯度合并：`model_training_paral_pruning.py:1387-1417`
- 集中式训练同步：`model_training.py:953-1001`、`model_training.py:1642-1652`
- 历史 bug 修复：`model_training_paral_pruning.py:1422-1426`、`FIXES_SUMMARY.md`
- 可视化改造：`main_MIA.py:4-6`、`main_test_MIA.py:4-13`
- 依赖文件：`requirements.txt`
- 数据生成工具：`generate_test_data.py`

---

## 9. 会议前行动清单
- [ ] 讨论 Slot Attention 降维方案并实现。
- [ ] 确定注意力模块的优化策略（训练或冻结）。
- [ ] 调整阈值逻辑，记录实验结果。
- [ ] 对比 GMM 路径，形成准确率/防御指标表格。
- [ ] 更新环境说明文件，准备在会后同步给团队。
