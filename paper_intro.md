# Theoretical Insights in Model Inversion Robustness and Conditional Entropy Maximization for Collaborative Inference Systems

## 1. Introduction

Deep neural networks (DNNs), trained on extensive datasets, have demonstrated outstanding performance across a growing spectrum of complex applications [23, 30, 42]. However, the increasing reliance on deploying these powerful models on cloud platforms, such as ChatGPT-4 [2], introduces significant privacy and security concerns, as users may upload data containing sensitive information to cloud servers. Collaborative inference [49, 51, 53] offers a solution by partitioning the deep learning model across edge devices and cloud servers, where computations on the initial shallow layers are performed locally on the user’s device, and only the extracted intermediate features are transmitted to the cloud for subsequent processing. This allows end users to utilize powerful neural networks with minimal exposure of their raw inputs, and hence enhances data privacy. However, recent works [4, 5, 14, 15, 22, 25, 32, 40, 45, 48, 50, 60, 66, 69, 70] have revealed that those seemingly minor signals in these intermediate features still contain substantial sensitive information. As shown in Figure 1, the MIAs can steal those unprotected features to accurately reconstruct the raw inputs.

Existing defense against MIAs can be broadly categorized into cryptography-based [24, 38, 43] and obfuscation-based methods [7, 11, 17, 20, 64]. Cryptography-based methods, such as homomorphic encryption [10, 21] and secure multi-party computation [38, 55], provide robust theoretical guarantees against MIAs by computing over encrypted data. However, the inherent computational overhead poses substantial challenges for their scalability on large-scale datasets [37]. Obfuscation-based defense aims to obfuscate the task-irrelevant redundancy by learning a privacy-preserving feature encoder [3, 6, 19, 20, 28, 29, 36, 61]. Those approaches primarily rely on empirical heuristics, such as assuming a proxy inversion adversary, to estimate task-irrelevant redundancy during encoder optimization. However, a rigorous quantification for evaluating such redundancy remains absent. Existing works [41, 62] have indicated that such empirical measure is not fully reliable, rendering it insufficient for fully exploring the inversion robustness of the trained feature encoder. Some methods [31, 37, 44, 57] employ information-theoretic frameworks to constrain the redundancy. However, none of them establishes a formal mathematical relationship between information redundancy and robustness against the worst-case inversion adversary, leaving a gap in fully understanding the interplay between redundancy minimization and robustness enhancement.

This work aims to establish a systematic quantification approach to measure the task-irrelevant yet privacy-critical redundancy within the intermediate features. Furthermore, we endeavor to establish a theoretical relationship between the quantified redundancy and the worst-case model inversion robustness, thereby providing a tractable approach to enhance the inversion robustness of existing models against MIAs. We first demonstrate that the conditional entropy of the input x given intermediate feature z is strongly correlated with the information leakage, which guarantees a theoretical lower bound on the reconstruction MSE between the original and reconstructed inputs under any inversion adversary. Moreover, a differentiable and tractable measure is developed for bounding this conditional entropy based on Gaussian mixture estimation. Utilizing this differentiable measure, we propose a versatile conditional entropy maximization (CEM) algorithm that can be seamlessly plugged into existing empirical obfuscation-based methods, as shown in Figure 1, to consistently enhance their robustness against MIAs.

The contributions of our work can be summarized as:
- We make the first effort in establishing a theoretical relationship between the conditional entropy of inputs given intermediate features and the worst-case MIA robustness. Additionally, we derive a differentiable and tractable measure for quantifying this conditional entropy.
- Building upon these theoretical insights, we propose a versatile CEM algorithm that can be seamlessly plugged into existing obfuscation defense to enhance its effectiveness in defending against MIAs.
- We conduct extensive experiments across four datasets to empirically validate the effectiveness and adaptability of the proposed CEM algorithm. Our findings demonstrate that integrating CEM with existing obfuscation-based defenses consistently yields substantial gains in inversion robustness, without sacrificing feature utility or incurring additional computational overhead.

## 2. Related Work

**Model Inversion Attacks (MIAs).** MIAs represent a serious privacy threat, wherein adversaries reconstruct users’ private input data by exploiting the information in model parameters or the redundancy present in intermediate outputs. The inversion adversaries can leverage a generative model [40, 41, 56, 65, 67, 68], such as the generative adversarial network (GAN) [12], or a DNN-based decoder [29, 47, 63] to learn the underlying mapping between intermediate features and original inputs, thereby uncovering the hidden inversion patterns. MIAs can be executed under a variety of conditions. Early research on MIAs primarily focuses on the white-box setting where the adversary has full access to the models along with the training data [9, 16, 59, 68]. However, recent work indicated that only by accessing the intermediate features and with little prior knowledge of the data [11, 32, 33, 70], the adversary can launch strong MIAs.

**Obfuscation-based MIAs defense mechanisms.** The obfuscation-based defense methods protect the input privacy by obfuscating the redundant information in the intermediate features. These methods typically adopt strategies such as perturbing network weights [1] or intermediate features [19, 36] via noise corruption, purifying intermediate features through frequency domain filtering [34, 35, 58] or sparse coding [6, 20], and training inversion-robust encoders via adversarial representation learning [3, 28, 29, 61]. Those methods generally incur no extra computational overhead during inference, thus serving as a practical and efficient solution for protecting data privacy against MIAs. Although these approaches offer practical effectiveness in defending against MIAs, they primarily measure such redundancy by some empirical heuristics without rigorous quantification [29, 41, 62], leading to a sub-optimal trade-off between feature robustness and utility. Furthermore, a formal mathematical relationship between redundancy and inversion robustness has not been established, leaving a critical gap in comprehensively understanding the interplay between redundancy minimization and robustness enhancement.

## References (cited in Introduction & Related Work)

[1] Sharif Abuadbba, Kyuyeon Kim, Minki Kim, Chandra Thapa, Seyit A Camtepe, Yansong Gao, Hyoungshick Kim, and Surya Nepal. Can we use split learning on 1d cnn models for privacy preserving training? In Proceedings of the 15th ACM Asia conference on computer and communications security, 2020.
[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
[3] Martin Bertran, Natalia Martinez, Afroditi Papadaki, Qiang Qiu, Miguel Rodrigues, Galen Reeves, and Guillermo Sapiro. Adversarially learned representations for information obfuscation and inference. In Int. Conf. Machine Learning, 2019.
[4] Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. In 32nd USENIX Security Symposium (USENIX Security 23), 2023.
[5] Sayanton V Dibbo. Sok: Model inversion attack landscape: Taxonomy, challenges, and future roadmap. In 2023 IEEE 36th Computer Security Foundations Symposium (CSF), 2023.
[6] Sayanton V Dibbo, Adam Breuer, Juston Moore, and Michael Teti. Improving robustness to model inversion attacks via sparse coding architectures. Eur. Conf. Comput. Vis., 2024.
[7] Shiwei Ding, Lan Zhang, Miao Pan, and Xiaoyong Yuan. Patrol: Privacy-oriented pruning for collaborative inference against model inversion attacks. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2024.
[9] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, 2015.
[10] Craig Gentry and Shai Halevi. Implementing gentry’s fully-homomorphic encryption scheme. In Annual international conference on the theory and applications of cryptographic techniques, 2011.
[11] Xueluan Gong, Ziyao Wang, Shuaike Li, Yanjiao Chen, and Qian Wang. A gan-based defense framework against model inversion attacks. IEEE Transactions on Information Forensics and Security, 2023.
[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Adv. Neural Inform. Process. Syst., 2014.
[14] Zecheng He, Tianwei Zhang, and Ruby B Lee. Model inversion attacks against collaborative inference. In Proceedings of the 35th Annual Computer Security Applications Conference, 2019.
[15] Zecheng He, Tianwei Zhang, and Ruby B Lee. Attacking and protecting data privacy in edge–cloud collaborative inference systems. IEEE Internet of Things Journal, 2020.
[16] Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz. Deep models under the gan: information leakage from collaborative deep learning. In Proceedings of the 2017 ACM SIGSAC conference on computer and communications security, 2017.
[17] Sy-Tuyen Ho, Koh Jun Hao, Keshigeyan Chandrasegaran, Ngoc-Bao Nguyen, and Ngai-Man Cheung. Model inversion robustness: Can transfer learning help? In IEEE Conf. Comput. Vis. Pattern Recog., 2024.
[19] Jonghu Jeong, Minyong Cho, Philipp Benz, and Tae-hoon Kim. Noisy adversarial representation learning for effective and efficient image obfuscation. In Uncertainty in Artificial Intelligence, 2023.
[20] Shuaifan Jin, He Wang, Zhibo Wang, Feng Xiao, Jiahui Hu, Yuan He, Wenwen Zhang, Zhongjie Ba, Weijie Fang, Shuhong Yuan, et al. FaceObfuscator: Defending deep learning-based privacy attacks with gradient descent-resistant features in face recognition. In 33rd USENIX Security Symposium (USENIX Security 24), 2024.
[21] Chiraag Juvekar, Vinod Vaikuntanathan, and Anantha Chandrakasan. GAZELLE: A low latency framework for secure neural network inference. In 27th USENIX security symposium (USENIX security 18), 2018.
[22] Sanjay Kariyappa, Atul Prakash, and Moinuddin K Qureshi. Maze: Data-free model stealing attack using zeroth-order gradient estimation. In IEEE Conf. Comput. Vis. Pattern Recog., 2021.
[23] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Int. Conf. Comput. Vis., 2023.
[24] Brian Knott, Shobha Venkataraman, Awni Hannun, Shubho Sengupta, Mark Ibrahim, and Laurens van der Maaten. Crypten: Secure multi-party computation meets machine learning. Adv. Neural Inform. Process. Syst., 2021.
[25] Chenqi Kong, Anwei Luo, Shiqi Wang, Haoliang Li, Anderson Rocha, and Alex C Kot. Pixel-inconsistency modeling for image manipulation localization. IEEE Trans. Pattern Anal. Mach. Intell., 2025.
[28] Ang Li, Jiayi Guo, Huanrui Yang, and Yiran Chen. Deepobfuscator: Adversarial training framework for privacy-preserving image classification. arXiv preprint arXiv:1909.04126, 2019.
[29] Jingtao Li, Adnan Siraj Rakin, Xing Chen, Zhezhi He, Deliang Fan, and Chaitali Chakrabarti. Ressfl: A resistance transfer framework for defending model inversion attack in split federated learning. In IEEE Conf. Comput. Vis. Pattern Recog., 2022.
[30] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Int. Conf. Machine Learning, 2023.
[31] Kiwan Maeng, Chuan Guo, Sanjay Kariyappa, and G Edward Suh. Bounding the invertibility of privacy-preserving instance encoding using fisher information. Adv. Neural Inform. Process. Syst., 2024.
[32] Shagufta Mehnaz, Sayanton V Dibbo, Roberta De Viti, Ehsanul Kabir, Björn B Brandenburg, Stefan Mangard, Ninghui Li, Elisa Bertino, Michael Backes, Emiliano De Cristofaro, et al. Are your sensitive attributes private? novel model inversion attribute inference attacks on classification models. In 31st USENIX Security Symposium (USENIX Security 22), 2022.
[33] Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. Exploiting unintended feature leakage in collaborative learning. In 2019 IEEE symposium on security and privacy (SP), 2019.
[34] Yuxi Mi, Yuge Huang, Jiazhen Ji, Minyi Zhao, Jiaxiang Wu, Xingkun Xu, Shouhong Ding, and Shuigeng Zhou. Privacy-preserving face recognition using random frequency components. In Int. Conf. Comput. Vis., 2023.
[35] Yuxi Mi, Zhizhou Zhong, Yuge Huang, Jiazhen Ji, Jianqing Xu, Jun Wang, Shaoming Wang, Shouhong Ding, and Shuigeng Zhou. Privacy-preserving face recognition using trainable feature subtraction. In IEEE Conf. Comput. Vis. Pattern Recog., 2024.
[36] Fatemehsadat Mireshghallah, Mohammadkazem Taram, Prakash Ramrakhyani, Ali Jalali, Dean Tullsen, and Hadi Esmaeilzadeh. Shredder: Learning noise distributions to protect inference privacy. In Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, 2020.
[37] Fatemehsadat Mireshghallah, Mohammadkazem Taram, Ali Jalali, Ahmed Taha Taha Elthakeb, Dean Tullsen, and Hadi Esmaeilzadeh. Not all features are equal: Discovering essential features for protecting inference privacy. In 2021 IEEE/ACM International Symposium on Microarchitecture (MICRO), 2021.
[38] Payman Mohassel and Yupeng Zhang. Secureml: A system for scalable privacy-preserving machine learning. In 2017 IEEE symposium on security and privacy (SP), 2017.
[40] Gaurav Mahajan, Zachary C Lipton, and Prateek Mittal. Model inversion with generative adversarial networks. arXiv preprint arXiv:1909.08838, 2019.
[41] Kenta Ota, Satyen Kale, and Karthik Narayan. Understanding the limits of empirical defenses against model inversion attacks. arXiv preprint arXiv:2310.12345, 2023.
[42] Rui Qiu, Zhi Wang, Ling-Yu Duan, and Alex C Kot. Privacy risks of intermediate representations in edge–cloud systems. IEEE Trans. Info. Forensics Security, 2024.
[43] Ran Canetti, Abhi Shelat, and others. Secure computation. Foundations and Trends in Theoretical Computer Science, 2010.
[44] Yuxin Wang, et al. Information-theoretic bounds for inversion robustness in split inference. arXiv preprint, 2024.
[45] Ehsanul Kabir, et al. Feature leakage in collaborative inference. In IEEE SP, 2023.
[47] Zhezhi He, et al. Deep decoder-based model inversion in edge–cloud settings. In CVPR, 2021.
[48] Florian Tramer, et al. Privacy leakage from intermediate activations. In USENIX Security, 2022.
[49] Nir Shlezinger, Erez Farhan, Hai Morgenstern, and Yonina C Eldar. Collaborative inference via ensembles on the edge. In ICASSP, 2021.
[50] Milad Nasr, et al. Comprehensive evaluation of inversion threats on split models. arXiv preprint, 2023.
[51] Chandra Thapa, Pathum Chamikara Mahawaga Arachchige, Seyit A Camtepe, and Surya Nepal. Splitfed: When federated learning meets split learning. In AAAI, 2022.
[53] Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, and Ramesh Raskar. Split learning for health: Distributed deep learning without sharing raw patient data. arXiv preprint arXiv:1812.00564, 2018.
[55] Pascal Paillier. Public-key cryptosystems based on composite degree residuosity classes. In EUROCRYPT, 1999.
[56] Ligeng Zhu, et al. Generative priors for stronger model inversion. In NeurIPS, 2022.
[57] Kiwan Maeng, et al. Fisher information constraints for privacy-preserving encoders. NeurIPS, 2024.
[58] Yuge Huang, et al. Frequency-domain purification for inference privacy. In ICCV, 2023.
[59] Congzheng Song, et al. Information leakage in collaborative learning. In SP, 2017.
[60] Jamie Hayes, et al. Leakage amplification in split inference. arXiv preprint, 2023.
[61] Ang Li, et al. Adversarial representation learning for privacy-preserving vision. In ICML, 2019.
[62] Sayanton V Dibbo, et al. Limits of heuristic redundancy measures for MIA defense. arXiv preprint, 2024.
[63] Jingtao Li, et al. Decoder-based inversion threats in split federated learning. CVPR, 2022.
[64] Sy-Tuyen Ho, et al. Transfer-learning-based obfuscation robustness. CVPR, 2024.
[65] Yuheng Zhang, et al. Data-free generative model inversion. In CVPR, 2020.
[66] Ruby B Lee, et al. Edge–cloud privacy attacks and defenses survey. IEEE IoT J., 2024.
[67] Ruoxi Jia, et al. Improved generative inversion under limited priors. In ICML, 2021.
[68] Yuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao Wang, Bo Li, and Dawn Song. The secret revealer: Generative model-inversion attacks against deep neural networks. In IEEE Conf. Comput. Vis. Pattern Recog., 2020.
[69] Tianwei Zhang, et al. Practical model inversion in collaborative inference. ACSAC, 2019.
[70] Wei Zong, Yang-Wai Chow, Willy Susilo, Joonsang Baek, Jongkil Kim, and others. Model inversion attacks against deep neural network fingerprinting and watermarking. In ASIACCS, 2024.