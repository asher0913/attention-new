digraph GatedAttCEMv6 {
    graph [
        rankdir=TB,
        fontsize=12,
        labelloc="t",
        label="Gated Attention CEM（与原报告一致）",
        pad="0.5",
        nodesep="0.5",
        ranksep="0.85"
    ];
    node [
        shape=rectangle,
        fontname="Helvetica",
        fontsize=11,
        style="filled,rounded",
        fillcolor="#F8F9F9",
        width=4.2
    ];
    edge [color="#4D5656", penwidth=1.3, arrowsize=0.85];

    input [shape=parallelogram, fillcolor="#D6EAF8",
        label=<
            <B>输入批次</B><BR ALIGN="LEFT"/>
            特征 <I>Z</I>（割点输出）+ 标签 <I>Y</I>
        >];

    group_class [label=<
        <B>按标签分组</B><BR ALIGN="LEFT"/>
        得到每个类的样本矩阵 <I>X</I><SUB>c</SUB>
    >, width=4.0];

    norm_stage [label=<
        <B>LayerNorm</B><BR ALIGN="LEFT"/>
        <I>\hat{X}</I><SUB>c</SUB> = LayerNorm(<I>X</I><SUB>c</SUB>)
    >, fillcolor="#E8F4F8", width=4.0];

    pooling [label=<
        <B>GatedAttentionPooling（报告 §2）</B><BR ALIGN="LEFT"/>
        <I>V</I> = tanh(<I>W</I><SUB>V</SUB><I>\hat{X}</I><SUB>c</SUB>), <I>U</I> = σ(<I>W</I><SUB>U</SUB><I>\hat{X}</I><SUB>c</SUB>)<BR ALIGN="LEFT"/>
        logits = <I>W</I><SUB>w</SUB>(<I>V</I> ⊙ <I>U</I>), <I>a</I> = softmax(logits)
    >, fillcolor="#FADBD8", width=4.6];

    mean_stage [label=<
        <B>加权均值</B><BR ALIGN="LEFT"/>
        <I>\mu</I><SUB>c</SUB> = Σ<sub>m</sub> <I>a</I><SUB>m</SUB> <I>\hat{x}</I><SUB>m</SUB>
    >, fillcolor="#E8DAEF", width=4.0];

    var_stage [label=<
        <B>加权方差</B><BR ALIGN="LEFT"/>
        <I>\sigma</I><SUB>c</SUB><SUP>2</SUP> = Σ<sub>m</sub> <I>a</I><SUB>m</SUB> (<I>\hat{x}</I><SUB>m</SUB> - <I>\mu</I><SUB>c</SUB>)²<BR ALIGN="LEFT"/>
        clamp 至 eps，避免 log0
    >, fillcolor="#E8DAEF", width=4.4];

    thresh_stage [label=<
        <B>阈值化 log-variance</B><BR ALIGN="LEFT"/>
        logσ² = log(<I>\sigma</I><SUP>2</SUP> + γ)<BR ALIGN="LEFT"/>
        ce_sur = max(0, logσ² - logσ²<sub>thr</sub>)
    >, fillcolor="#FDEBD0", width=4.4];

    class_stage [label=<
        <B>类级输出</B><BR ALIGN="LEFT"/>
        <I>L</I><SUB>c</SUB> = mean(ce_sur), MSE<sub>c</sub> = mean‖<I>\hat{x}</I> - <I>\mu</I>‖²<BR ALIGN="LEFT"/>
        权重 <I>w</I><SUB>c</SUB> = <I>M</I>/<I>B</I>
    >, fillcolor="#FEF9E7", width=4.6];

    outputs [shape=hexagon, fillcolor="#FCF3CF",
        label=<
            <B>聚合</B><BR ALIGN="LEFT"/>
            rob_loss = Σ<sub>c</sub> <I>w</I><SUB>c</SUB> <I>L</I><SUB>c</SUB><BR ALIGN="LEFT"/>
            intra_mse = Σ<sub>c</sub> <I>w</I><SUB>c</SUB> MSE<sub>c</sub>
        >, width=4.4];

    train_stage [label=<
        <B>训练整合</B><BR ALIGN="LEFT"/>
        · rob_loss × attention_loss_scale<BR ALIGN="LEFT"/>
        · 先反向 rob_loss，缓存梯度<BR ALIGN="LEFT"/>
        · 再反向主任务并合并梯度<BR ALIGN="LEFT"/>
        · NaN/Inf → rob_loss 置零
    >, fillcolor="#FDEBD0", width=4.8];

    input -> group_class -> norm_stage -> pooling -> mean_stage -> var_stage -> thresh_stage -> class_stage -> outputs -> train_stage;
}
