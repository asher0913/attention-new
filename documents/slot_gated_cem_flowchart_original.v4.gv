digraph SlotGatedCEMv4 {
    graph [
        rankdir=TB,
        fontsize=12,
        labelloc="t",
        label="Slot + Gated Cross Attention CEM 全流程（中文详解 v4）",
        pad="0.4"
    ];
    node [
        shape=rectangle,
        fontname="Noto Sans CJK SC",
        fontsize=11,
        style=filled,
        fillcolor="#F9FAFB",
        width=3.6,
        height=1.0
    ];
    edge [color="#4C4C4C", penwidth=1.4, arrowsize=0.85];

    input [shape=parallelogram, fillcolor="#D9EDF7",
           label="编码器输出 Z ∈ ℝ^{B×D}\n对应标签 Y；batch 内含多类样本"];

    subgraph cluster_pre {
        label="阶段 0：按类构建序列（源码：SlotCrossAttentionCEM.forward 开头）";
        fontsize=12;
        style="rounded";
        color="#AED6F1";
        group_cls [label="对每个类别 c：X_c = {x_m | y_m = c}\n若 M=|X_c|≤2 直接跳过，保持与代码一致"];
        norm_proj [label="对 X_c 做 LayerNorm 和线性变换：\nK_m = W_k·LN(x_m), V_m = W_v·LN(x_m)\n（model_training_paral_pruning.py 第 36-60 行）", fillcolor="#E8F8F5"];
        note_pre [shape=note, fontname="Noto Sans CJK SC", label="目的：让后续 Slot Attention 只看到同一类别，避免跨类干扰。\n同时保证张量形状 [1,M,D]，与源码 tokens 构造一致。"];
    }

    subgraph cluster_slot {
        label="阶段 1：Slot Attention（定位类内子模态，源码第 61-126 行）";
        fontsize=12;
        style="rounded";
        color="#F5B7B1";
        slot_init [label="参数化初始化：s_k^{(0)} = μ + σ ⊙ ε,\nS=8（可调）；μ,σ 为 nn.Parameter"];
        slot_iter [label="迭代 t=1..T=3：\n1) q_k = W_q·LN(s_k^{(t-1)})/√d\n2) α_{mk} = softmax_k(K_m^⊤ q_k / τ), τ=d^{-1/4}\n3) u_k = Σ_m α_{mk} V_m\n4) s_k^{(t)} = GRU(u_k, s_k^{(t-1)}) + MLP(LN(·))", fillcolor="#FADBD8", width=4.1];
        slots_out [label="输出 S 个槽 {s_k} 视作“可学习混合中心”\n（对应 GMM 中的子簇，但可端到端训练）", fillcolor="#FDEDEC"];
        note_slot [shape=note, label="源码中的 self.slot_attention(tokens) 完成此步骤\n并在日志中打印迭代次数与温度。"];
    }

    subgraph cluster_cross {
        label="阶段 2：Flamingo 式门控交叉注意（源码 CrossAttention）";
        fontsize=12;
        style="rounded";
        color="#ABEBC6";
        cross_norm [label="查询：q_m = LN_q(x_m)\n键值：K_s = LN_kv(s_s), V_s = LN_kv(s_s)", fillcolor="#E8F8F5"];
        cross_attn [label="多头交叉注意：\nα_m = softmax(q_m K^⊤ / √d)，h_m = α_m V\n输出 o_m = W_o h_m", width=4.0];
        gate_res [label="门控残差：\ny_m = x_m + tanh(α_attn)·o_m\n y_m = y_m + tanh(α_ffn)·FFN(LN_ff(y_m))\n（α 参数学习，初值 0.1）", fillcolor="#D5F5E3", width=4.1];
        enhanced [label="增强特征 \\tilde{x}_m = y_m\n用于后续统计与 MSE 监控", fillcolor="#E8F8F5"];
        note_cross [shape=note, label="源码中 self.cross_attention(class_feats, slots)\n并在 warmup 结束后加入主优化器。"];
    }

    subgraph cluster_mix {
        label="阶段 3：槽职责 + 多级门控统计（源码 SlotCrossAttentionCEM.forward 中段）";
        fontsize=12;
        style="rounded";
        color="#D2B4DE";
        cosine [label="归一化余弦：s_{ms} = ⟨\\tilde{x}_m/||·||, s_s/||·||⟩"];
        resp [label="软职责：r_{ms} = softmax_s(β · s_{ms})\nβ = self.assign_temp（默认 12）"];
        slot_mass [label="槽质量：w_s = Σ_m r_{ms}\n强化：\\tilde{w}_s = (w_s/M)^{γ} / Σ_j (w_j/M)^{γ}", width=4.0];
        slot_stats [label="槽均值/方差：\nμ_s = (Σ_m r_{ms} \\tilde{x}_m)/w_s\nσ_s² = Σ_m r_{ms}(\\tilde{x}_m-μ_s)²/w_s\n（代码中使用 torch.einsum 实现）", fillcolor="#F5EEF8", width=4.3];
        gate_dim [label="维度门：g_soft = sigmoid(MLP(LN(log σ_s²)))\nSNR 门：g_snr = sigmoid(κ (σ_s²/(μ_s²+ε) - τ_snr))\nSoftplus：φ = softplus(β'(log σ_s² - log θ) - m)/β'", fillcolor="#EBDEF0", width=4.4];
        agg_dim [label="按槽聚合：logvar_d = Σ_s \\tilde{w}_s g_soft g_snr φ\n类级门：g_c = sigmoid(a(M/B - b))\nlogvar_c = g_c · mean_d(logvar_d)", fillcolor="#F5EEF8", width=4.4];
        mse [label="附加监控：MSE_c = (1/M) Σ_m ||\\tilde{x}_m - μ_c||²\n仅记录到日志，不参与梯度", fillcolor="#E8DAEF"];
        note_mix [shape=note, label="对应源码 gate_mlp、snr_sharp、softplus_beta 等超参；\nEarly shutoff 会在平均 gate 超阈值时短路。"];
    }

    subgraph cluster_loss {
        label="阶段 4：CEM surrogate 与训练融合（源码 train_target_step）";
        fontsize=12;
        style="rounded";
        color="#F8C471";
        rob_loss [shape=hexagon, fillcolor="#FCF3CF",
                  label="rob_loss = Σ_c p_c · max(0, logvar_c - log τ_glob)\np_c = M_c / B，τ_glob = var_threshold·reg_strength² + γ", width=4.3];
        scale [label="rob_loss ← attention_loss_scale × rob_loss\n（默认 0.25，可在 config 中调节）", fillcolor="#FDEBD0"];
        grad_cache [label="先对 rob_loss backward(retain_graph=True)\n缓存 encoder & attention 梯度，随后 optimizer.zero_grad()", fillcolor="#FDEBD0", width=4.3];
        fuse [label="再计算主任务 loss（CE/重建等），backward()\n最后：param.grad += λ·grad_rob（带 LR 缩放）\n注意力参数恢复缓存梯度后一起 step()", fillcolor="#FDEBD0", width=4.4];
        guard [shape=note, label="启用条件：current_epoch>warmup(3)，λ>0，非 random_ini_centers。\n若检测到 NaN/Inf 或 gate 统计异常 → rob_loss 置零但保留计算图。"];
    }

    input -> group_cls -> norm_proj -> slot_init -> slot_iter -> slots_out -> cross_norm -> cross_attn -> gate_res -> enhanced -> cosine -> resp -> slot_mass -> slot_stats -> gate_dim -> agg_dim -> rob_loss -> scale -> grad_cache -> fuse;
    agg_dim -> mse;
    norm_proj -> note_pre;
    slot_iter -> note_slot;
    gate_res -> note_cross;
    gate_dim -> note_mix;
    fuse -> guard;
}
