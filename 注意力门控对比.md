# 注意力门控与原始 GMM 条件熵代理对比说明

## 1. 背景

原始 `CEM-main` 工程中，条件熵近似采用 **KMeans + GMM** 的流程：  
- 每个 epoch 结束后收集全部中间表征；  
- 先用 KMeans 生成初始簇中心，再用 GMM 拟合类内多模态分布；  
- 通过各簇协方差矩阵的对数行列式构造条件熵惩罚 `rob_loss`。  

在 `gated-att` 分支中，我们尝试用 **门控注意力 (Gated Attention)** 替换上述流程：  
- 对同类样本直接在 mini-batch 内计算门控注意力权重；  
- 用加权均值和加权方差估计类内扩散程度；  
- 以阈值化后的对数方差作为新的条件熵 surrogate。

## 2. 主要代码差异

### 2.1 条件熵代理模块

| 项目 | 原始 `CEM-main` | `gated-att` |
| ---- | --------------- | ----------- |
| 入口函数 | `compute_class_means()` | `GatedAttentionCEM.forward()` |
| 依赖 | `GMM.py`、KMeans、PCA | 无外部聚类，使用 `GatedAttentionPooling` |
| 统计特征 | 多簇均值、协方差、混合权重 | 注意力权重下的加权均值和方差 |
| 惩罚形式 | `log det(Σ)` − 阈值 | `log(var)` − 阈值 |
| 执行时机 | 每 epoch 额外遍历全量特征 | 训练步骤内即时计算 |

### 2.2 训练流程

1. **原始实现 (`CEM-main`)**
   - `train_target_step` 每次返回交叉熵损失，并在非随机初始化阶段调用 `compute_class_means`。  
   - `rob_loss` 首先反传记录 encoder 梯度，之后与交叉熵梯度加权合并。  
   - 额外在 epoch 尾部写入日志、保存 KMeans/KMeans++ 结果和 t-SNE 可视化。

2. **门控注意力实现 (`gated-att`)**
   - 删除 KMeans/GMM 全部函数及其缓存（`centroids_list` 等）。  
   - 在 `train_target_step` 中直接创建或复用 `GatedAttentionCEM`。  
   - 引入 **warm-up**（默认前 5 个 epoch 关闭门控）与 **loss scale**（默认 0.1）避免早期梯度不稳定。  
   - 首次实例化后把门控注意力参数加入全局优化器，使其可学习。

### 2.3 附属脚本与依赖

- `run_exp.sh` 重新启用 `cd "$(dirname "$0")"`，确保脚本在 `gated-att` 目录运行；  
- `main_MIA.py`、`main_test_MIA.py` 仅保留对 `model_training_paral_pruning` 的引用；  
- 移除 `GMM.py`、`model_training_GMM.py`、`model_training.py`，新增文档 `GATED_ATT_CHANGES.md`。

## 3. 目前问题与注意事项

- 由于门控注意力版本尚未完全稳定，训练过程中 **分类准确率可能停留在约 10%**，表明 encoder 输出被压缩为近似常数。  
- 造成该现象的主要原因：门控正则梯度过强或注意力参数仍未得到充分训练。  
- 如果需要恢复原始行为，可从 `CEM-main` 复制 `compute_class_means` 以及相关聚类逻辑回 `gated-att`，或直接回退至 `CEM-main` 分支。

## 4. 下一步建议

1. **排查梯度路径**：确认 gated attention 参数在优化器中确实被更新，并通过梯度范数监控其是否有效。  
2. **逐步调参**：尝试增大 warm-up epoch，降低 `attention_loss_scale`，或为注意力模块单独设定较小学习率。  
3. **对照验证**：在同一 Linux 环境下分别运行 `CEM-main` 与 `gated-att`，以交叉熵与 `rob_loss` 轨迹、攻击指标为基准衡量两种代理的差异。  
4. **若需可用版本**：暂时建议继续使用原始 GMM 代理，待门控注意力调试稳定后再做替换。

---

如需进一步的差异清单或恢复脚本，可参考 `GATED_ATT_CHANGES.md` 或直接从 `CEM-main` 复制对应模块。
