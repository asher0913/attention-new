# gated-att 与 CEM-main 逐行代码对比

本文逐文件逐段列出 `gated-att/` 与 `CEM-main/` 的全部差异，并在每段代码后给出修改原因。最后附整体总结。

---

## 一、目录级差异

- **删除文件**
  - `GMM.py`：原条件熵代理（KMeans+GMM）的实现文件。
  - `model_training_GMM.py`：旧版训练脚本，专门服务于 GMM 条件熵。
  - `model_training.py`：早期备用训练入口。
- **新增文件**
  - `GATED_ATT_CHANGES.md`：迁移说明文档。
  - `门控注意力逐行对比.md`：本文档。

---

## 二、逐文件差异

### 1. `run_exp.sh`

#### 代码差异

```diff
@@
-# cd "$(dirname "$0")"
-# cd ../../
+cd "$(dirname "$0")"
```

#### 改动原因
- 重新启用 `cd "$(dirname "$0")"`，确保在 Linux 上直接执行脚本时会切换到 `gated-att` 目录运行，避免误调用其它工程。

---

### 2. `main_MIA.py`

#### 代码差异

```diff
@@
-import model_training,model_training_paral_pruning
+import model_training_paral_pruning
```

#### 改动原因
- 删除对已移除的 `model_training.py` 的引用，只保留门控注意力版本的训练逻辑。

---

### 3. `main_test_MIA.py`

#### 代码差异

```diff
@@
-import model_training,model_training_paral_pruning
+import model_training_paral_pruning
```

#### 改动原因
- 与训练脚本保持一致，去除对旧文件的依赖，避免导入报错。

---

### 4. `model_training_paral_pruning.py`

以下 12 段为全部差异的逐段对比。

#### Hunk 0：文件头部导入与门控注意力类

```diff
@@ -23,18 +23,119 @@ import os
 import time
 from ptflops import get_model_complexity_info
 from shutil import rmtree
-from GMM import fit_gmm_torch
 from torchsummary import summary
 from sklearn.manifold import TSNE
 import torch_pruning as tp
 from datasets_torch import get_cifar100_trainloader, get_cifar100_testloader, get_cifar10_trainloader, \
     get_cifar10_testloader, get_mnist_bothloader, get_facescrub_bothloader, get_SVHN_trainloader, get_SVHN_testloader, get_fmnist_bothloader, get_tinyimagenet_bothloader,get_imagenet_bothloader,get_celeba_trainloader,get_celeba_testloader
-from sklearn.mixture import GaussianMixture
-# from cuml.mixture import GaussianMixture as cuGaussianMixture
-from sklearn.decomposition import PCA
-from sklearn.cluster import KMeans
 from torch.optim.lr_scheduler import CosineAnnealingLR
 from joblib import Parallel, delayed
+
+
+class GatedAttentionPooling(nn.Module):
+    """
+    Implementation follows the gated attention mechanism from
+    "Attention-based Deep Multiple Instance Learning" (Ilse et al., 2018).
+    """
+
+    def __init__(self, feature_dim: int, hidden_dim: int = 128):
+        super().__init__()
+        self.attention_V = nn.Linear(feature_dim, hidden_dim)
+        self.attention_U = nn.Linear(feature_dim, hidden_dim)
+        self.attention_w = nn.Linear(hidden_dim, 1)
+
+    def forward(self, features: torch.Tensor) -> torch.Tensor:
+        """
+        Args:
+            features: [M, D]
+        Returns:
+            attention_weights: [M, 1] softmax-normalized across M
+        """
+        V = torch.tanh(self.attention_V(features))
+        U = torch.sigmoid(self.attention_U(features))
+        logits = self.attention_w(V * U)  # [M, 1]
+        weights = torch.softmax(logits, dim=0)
+        return weights
+
+
+class GatedAttentionCEM(nn.Module):
+    """
+    Gated-Attention surrogate for Conditional Entropy Minimization.
+    Replaces the original KMeans/GMM surrogate with a lightweight
+    attention-based aggregation per class.
+    """
+
+    def __init__(self, feature_dim: int, hidden_dim: int = 128,
+                 var_threshold: float = 0.1, reg_strength: float = 0.0,
+                 eps: float = 1e-6):
+        super().__init__()
+        self.feature_dim = feature_dim
+        self.var_threshold = var_threshold
+        self.reg_strength = reg_strength
+        self.eps = eps
+        self.pool = GatedAttentionPooling(feature_dim, hidden_dim)
+        self.ln = nn.LayerNorm(feature_dim)
+
+    def forward(self, features: torch.Tensor, labels: torch.Tensor, unique_labels: torch.Tensor):
+        """
+        Args:
+            features: [B, D]
+            labels: [B]
+            unique_labels: sorted unique labels (torch.unique)
+        Returns:
+            rob_loss: scalar tensor
+            intra_mse: scalar tensor
+        """
+        device = features.device
+        dtype = features.dtype
+        total_weight = torch.zeros((), device=device, dtype=dtype)
+        total_logvar = torch.zeros((), device=device, dtype=dtype)
+        total_mse = torch.zeros((), device=device, dtype=dtype)
+
+        gamma = 1e-6
+        logvar_threshold = torch.tensor(
+            max(self.var_threshold * (self.reg_strength ** 2), 1e-8) + gamma,
+            device=device,
+            dtype=dtype,
+        )
+
+        for cls in unique_labels:
+            mask = (labels == cls)
+            class_feats = features[mask]  # [M, D]
+            M = class_feats.size(0)
+            if M <= 1:
+                continue
+
+            class_feats = self.ln(class_feats)
+            att_weights = self.pool(class_feats)  # [M, 1]
+            att_weights = att_weights.squeeze(-1)  # [M]
+
+            # Weighted statistics
+            weighted_mean = torch.sum(att_weights.unsqueeze(-1) * class_feats, dim=0)
+            diffs = class_feats - weighted_mean.unsqueeze(0)
+            var = torch.sum(att_weights.unsqueeze(-1) * diffs * diffs, dim=0)
+            var = torch.clamp(var, min=self.eps)
+
+            logvar = torch.log(var + gamma)
+            ce_sur = F.relu(logvar - torch.log(logvar_threshold))
+            logvar_c = ce_sur.mean()
+
+            mse_c = F.mse_loss(class_feats, weighted_mean.expand_as(class_feats))
+
+            weight = torch.tensor(float(M) / float(features.size(0)),
+                                  device=device, dtype=dtype)
+            total_weight += weight
+            total_logvar += logvar_c * weight
+            total_mse += mse_c * weight
+
+        if total_weight > 0:
+            rob_loss = total_logvar / total_weight
+            intra_mse = total_mse / total_weight
+        else:
+            rob_loss = torch.zeros((), device=device, dtype=dtype)
+            intra_mse = torch.zeros((), device=device, dtype=dtype)
+
+        return rob_loss, intra_mse
```

**改动原因**
- 移除 GMM/KMeans/PCA 等依赖，为门控注意力腾出空间。
- 新增 `GatedAttentionPooling` 与 `GatedAttentionCEM`，构建注意力版条件熵代理。

#### Hunk 1：构造函数新增 `current_epoch`

```diff
@@ -196,6 +297,7 @@ class MIA_train: # main class for every thing
         self.writer = SummaryWriter(log_dir=tensorboard_path)
         
         self.warm = 1
+        self.current_epoch = 0
         self.scheme = scheme
```

**改动原因**
- 记录当前 epoch，供门控注意力 warm-up 使用。

#### Hunk 2：新增门控注意力运行参数

```diff
@@ -330,6 +432,13 @@ class MIA_train: # main class for every thing
         
         ''' Activation Defense (end)'''

+        # Switch from GMM/KMeans surrogate to gated-attention surrogate
+        self.gated_attention_cem = None
+        self._gated_att_registered = False
+        self.attention_warmup_epochs = 5
+        self.attention_loss_scale = 0.1
+        self.global_train_step = 0
+
```

**改动原因**
- 初始化门控注意力实例，并设置延迟启用、loss 缩放及训练步计数器。

#### Hunk 3：移除 GMM 相关函数与参数，并改写 `train_target_step` 签名

```diff
@@ -617,417 +726,13 @@ class MIA_train: # main class for every thing
     def gan_scheduler_step(self, epoch = 0):
         for i in range(len(self.gan_scheduler_list)):
             self.gan_scheduler_list[i].step(epoch)
-    def apply_gmm_with_pca_and_inverse_transform(self,class_features, n_components=3, pca_components=100, iteration=30,ini_center=None):
-        ...
-    def kmeans_plusplus_init(self, X, num_clusters):
-        ...
-    def kmeans_cuda(self,X, num_clusters,centroids,random_ini_centers, num_iterations=10, tol=1e-4):
-        ...
-    def compute_class_means(self, features, labels, unique_labels, centroids_list, weights_list,cluster_variances_list):
-        ...
-    def train_target_step(self, x_private, label_private, adding_noise,random_ini_centers,centroids_list,weights_list,cluster_variances_list,client_id=0):
+    def train_target_step(self, x_private, label_private, adding_noise, random_ini_centers, client_id=0):
         self.f_tail.train()
         self.classifier.train()
         self.f.train()
         x_private = x_private.cuda()
         label_private = label_private.cuda()
+        self.global_train_step += 1
```

**改动原因**
- 删除所有 KMeans/GMM 辅助函数与返回值。
- 更新 `train_target_step` 签名，不再传递 `centroids_list` 等缓存。
- 增加 `global_train_step` 统计，为后续调试观察提供依据。

#### Hunk 4：条件熵计算改为门控注意力

```diff
@@ -1042,12 +747,60 @@ class MIA_train: # main class for every thing

-        if not random_ini_centers and self.lambd>0:
-            rob_loss,intra_class_mse = self.compute_class_means(...)
+        device = z_private.device
+        use_attention = (
+            not random_ini_centers
+            and self.lambd > 0
+            and self.current_epoch > self.attention_warmup_epochs
+        )
+        if use_attention:
+            ...
+            if self.gated_attention_cem is None:
+                ...
+                if not self._gated_att_registered:
+                    base_group = self.optimizer.param_groups[0]
+                    self.optimizer.add_param_group({
+                        'params': self.gated_attention_cem.parameters(),
+                        'lr': base_group.get('lr', self.lr),
+                        'momentum': base_group.get('momentum', 0.0),
+                        'weight_decay': base_group.get('weight_decay', 0.0),
+                    })
+                    self._gated_att_registered = True
+            else:
+                self.gated_attention_cem.train()
+            rob_loss, intra_class_mse = self.gated_attention_cem(...)
+            rob_loss = rob_loss * self.attention_loss_scale
+            ...
         else:
-            rob_loss,intra_class_mse=torch.tensor(0.0),torch.tensor(0.0)
+            rob_loss = torch.zeros((), device=device)
+            intra_class_mse = torch.zeros((), device=device)
         # assert 1==0, print(x_private.shape,label_private.shape,unique_values)
         # Final Prediction Logits (complete forward pass)
+        encoder_gradients = {}
```

**改动原因**
- 替换原 `compute_class_means` 调用，使用门控注意力直接计算 `rob_loss`。
- 加入 warm-up 判断及 loss 缩放，避免训练初期不稳定。
- 首次创建时把注意力参数加入优化器，并确保参与学习。
- 增加 NaN/Inf 防护，避免异常传播。

#### Hunk 5：反传前检查 `rob_loss` 是否有梯度

```diff
@@ -1170,7 +923,7 @@ class MIA_train: # main class for every thing
-        if not random_ini_centers and self.lambd>0:
+        if not random_ini_centers and self.lambd>0 and rob_loss.requires_grad:
```

**改动原因**
- 当门控注意力被跳过或返回零张量时，避免对无梯度的张量调用 `backward`。

#### Hunk 6：只在缓存到 encoder 梯度时叠加

```diff
@@ -1179,7 +932,7 @@ class MIA_train: # main class for every thing
-        if not random_ini_centers and self.lambd>0:
+        if not random_ini_centers and self.lambd>0 and encoder_gradients:
```

**改动原因**
- 防止在 attention 未运行时访问空字典导致错误。

#### Hunk 7：去除 epoch 级别的 GMM 缓存

```diff
@@ -1650,15 +1403,13 @@ class MIA_train: # main class for every thing
-            centroids_list= [torch.tensor(float('nan')) for _ in range(self.num_class)]
-            weights_list= [torch.tensor(float('nan')) for _ in range(self.num_class)]
-            cluster_variances_list=[torch.tensor(float('nan')) for _ in range(self.num_class)]
             ...
-            for epoch in range(1, self.n_epochs+1):
+            for epoch in range(1, self.n_epochs+1):
+                self.current_epoch = epoch
```

**改动原因**
- 不再维护聚类缓存；记录 epoch 进入 warm-up 判断。

#### Hunk 8：删除 `Z_all`/`label_all` 聚合

```diff
@@ -1672,8 +1423,6 @@ class MIA_train: # main class for every thing
-                Z_all = []
-                label_all = [] 
```

**改动原因**
- 全量特征缓存只为 GMM 服务，门控注意力不再需要。

#### Hunk 9：训练步骤调用修改

```diff
@@ -1715,7 +1464,9 @@ class MIA_train: # main class for every thing
-                            train_loss, f_loss, z_private = self.train_target_step(images, labels, adding_noise,random_ini_centers,centroids_list,weights_list,cluster_variances_list,client_id)
+                            train_loss, f_loss, z_private = self.train_target_step(
+                                images, labels, adding_noise, random_ini_centers, client_id
+                            )
```

**改动原因**
- 适配新签名，取消 GMM 相关参数。

#### Hunk 10：删除 epoch 末尾的聚类、日志、t-SNE

```diff
@@ -1771,176 +1522,6 @@ class MIA_train: # main class for every thing
-                if self.dataset != "imagenet" and self.regularization_strength!=0:
-                    feature_infer_stime= time.time()
-                    ...
-                    del Z_all,label_all
-
-                # kmeans_cuda(self,X, num_clusters,centroids, num_iterations=10, tol=1e-4):
-                
                 # V1/V2 synchronization
                 if self.scheme == "V1_epoch" or self.scheme == "V2_epoch":
```

**改动原因**
- 门控注意力不再需要每 epoch 的特征收集与聚类分析，相应日志与可视化全部移除。

#### Hunk 11：文件末尾换行

```diff
@@ -3126,4 +2707,4 @@ class MIA_train: # main class for every thing
         summary(model.cuda(), input_size=(3,32, 32))
         print('the param before pruning is:',base_nparams,'the param after pruning is:', nparams)
-        return model
\ No newline at end of file
+        return model
```

**改动原因**
- 添加缺失的换行，满足文本文件规范。

---

## 三、总结

1. **条件熵替换**：删掉全部 GMM/KMeans 逻辑，引入门控注意力模块即时计算加权方差，降低实现复杂度。
2. **训练流程调整**：`train_target_step` 只接受必要参数，逐步启用注意力惩罚，并在第一次使用时把注意力参数纳入优化器。
3. **辅助脚本同步**：`run_exp.sh`、`main_MIA.py`、`main_test_MIA.py` 都移除了旧依赖，确保脚本指向新实现。
4. **清理与规范**：删除所有与旧代理相关的缓存、日志、可视化，补齐文件末尾换行，保持代码整洁。

目前门控注意力版本仍需调参以恢复分类性能；若要立即获得稳定结果，可回退到 `CEM-main` 原始 GMM 方案或在此基础上继续调试超参数、梯度路径。

-----

我把两套代码逐行 diff 过，确认目前 gated-att 里唯一和训练梯度相关的改动，就是把原来的 compute_class_means / KMeans+GMM 逻辑换成 GatedAttentionCEM，以及在 train_target_step 里根据 use_attention 计算 rob_loss。其它诸如优化器调度、self.optimizer_zero_grad()/self.optimizer_step()、交叉熵正向与反传的顺序，都和 CEM-main 原始实现保持一致。

从日志看，cross-entropy loss 一直是 2.3026，Prec@1 稳定在 10%，说明编码器与分类器的参数几乎没有更新。这通常只会发生在梯度始终为 0 的情况下，所以怀疑仍是门控替换时把梯度链路弄断了。结合代码的执行路径，最可疑的环节有两个：

1. **门控模块没有真正参与优化**
   GatedAttentionCEM 是在 train_target_step 内按需动态创建的，虽然我们把它通过 add_param_group 加进了 self.optimizer，但如果 warm-up 期间（epoch <= attention_warmup_epochs）一直返回常量 torch.zeros()，就不会触发 rob_loss.backward()，随后的 self.optimizer_zero_grad() 也不会执行，最后 total_loss.backward() 虽然会产生交叉熵梯度，但我们没有看到它是否真的作用在 self.f / self.classifier 上。
   **建议先做一个最小化测试**：把 use_attention 固定为 False，或直接在 train_target_step 里把 self.lambd = 0，确保整条路径与原始 CEM 在“没有 CEM 惩罚”时完全等价，看交叉熵能否下降。如果仍旧 10%，说明问题不在门控模块本身，而是在某处梯度被清零。
2. **交叉熵梯度被错误地清理掉**
   现在 rob_loss.backward() 之后依旧调用了 self.optimizer_zero_grad()，这是为了保留 encoder 梯度并清空其它参数。然而当 rob_loss 的图并未构建（即 rob_loss.requires_grad == False）时，这段逻辑不会运行；但如果 rob_loss 是常量 0，却被 PyTorch 视为需要梯度（例如参与过一次带梯度的运算），self.optimizer_zero_grad() 仍会执行，把交叉熵的梯度清掉。可以在训练时打印 rob_loss.requires_grad 和 self.classifier.weight.grad.norm()，确认在 warm-up 阶段是否已经出现这种情况。

------

### 下一步建议

1. **做空白对照**
   先在 gated-att 里把门控完全关掉（例如强制 use_attention = False，或把 lambd 临时设为 0），确认在没有条件熵惩罚时，分类是否能恢复；这一步能快速区分是门控逻辑本身的问题，还是其它改动导致梯度断了。
2. **打印梯度范数**
   在 train_target_step 中 total_loss.backward() 之后，打印一两个关键参数（如 self.classifier.weight、self.f_tail 第一层卷积）的 grad.norm()，看看是否始终为 0；若是，则说明某处把梯度清掉了，需要沿着 self.optimizer_zero_grad() 和参数分组逻辑再排查。
3. **确认优化器分组**
   如果确实要动态加入门控参数，建议在初始化阶段（例如 __init__ 或 warm-up 结束时）一次性完成 self.optimizer.add_param_group()，避免在训练循环里频繁改动优化器内部状态；也可以考虑单独给门控 attention 建一个轻量的 torch.optim.Adam，和现有的 SGD 分开，以减少干扰。
4. **保留 GMM 作为 fallback**
   在门控彻底调通之前，建议保留一份原始的 compute_class_means（哪怕只是临时调用）来确保分类链路没有问题，再逐步插入门控逻辑。这样至少能先验证“没有条件熵”时训练是正常的。

目前根据代码 diff 判断，除了门控替换外其余逻辑与原始工程一致；准确率持续 10% 说明梯度确实没回到模型里，需要按上述步骤确认是哪一环节把梯度清空或阻断了。建议优先进行第 1、2 项测试，这样可以快速定位问题所在。

---

我刚刚直接在 gated-att/model_training_paral_pruning.py 里改了 train_target_step 的反传流程，确保门控注意力的梯度不会在 self.optimizer_zero_grad() 之后被清掉：

- 先把 GatedAttentionCEM 的参数列表和梯度保存下来；
- 跟原来一样对 rob_loss 反传并缓存 encoder 梯度，然后清零其它参数的梯度；
- 对 total_loss 反传后，把 encoder 的梯度叠加回去，再把刚才存下来的门控 attention 梯度重新写回去；
- 最后返回的训练损失改成 total_loss（仍然是标量 float）。

这样 rob_loss 的梯度既能调 encoder，也能真正更新门控注意力的权重。请把 lambda 恢复到原设置重新训练，观察准确率是否还会卡在 10%；如果梯度恢复正常但门控仍然压得太狠，可以再调小 attention_loss_scale 或 lambda 做微调。

---

主要是修正了门控注意力那部分梯度在反传时被清掉的问题。具体来说是修改了 gated-att/model_training_paral_pruning.py 中 train_target_step 函数的反向传播流程：

1. **在计算 rob_loss 之后保存梯度**
   - 先对 rob_loss 调用 backward(retain_graph=True)，拿到 encoder 的梯度，**同时把 GatedAttentionCEM 参数的梯度也克隆保存**。
   - 这样做的目的是保留“条件熵惩罚”对 encoder 和注意力模块的贡献。
2. **清零梯度后反传分类损失**
   - 调用 self.optimizer_zero_grad() 把刚产生的梯度清掉。
   - 再对 total_loss 也就是交叉熵（外加其它正则）做一次 backward()，得到正常分类链路的梯度。
3. **把两份梯度重新叠加**
   - 对 encoder 参数：把刚才保存的 rob_loss 梯度按 λ 加回去。
   - 对注意力参数：把保存的梯度重新写回去，否则上一阶段清零后就再也没有梯度了。

这一连串修复，保证了门控注意力在 λ>0 时仍能把梯度传到 encoder 和自身体重，分类损失也不会被 self.optimizer_zero_grad() 清成零，于是恢复了正常训练。简单说，就是确保“条件熵梯度”和“分类梯度”都能在一次迭代里叠加到相应参数上，所以把 lambda 调回 16 之后也能收敛。

---

目前的门控注意力版在 λ>0 时已经能恢复正常梯度流和分类收敛，所以**“分类 + 条件熵惩罚”** 这两条路现在都会同时发挥作用。不过它和 CEM-main 的 GMM/KMeans surrogate 还是有两点差异，需要注意：

- **条件熵近似方式不同**：门控注意力只用单层注意力 + 加权方差来衡量类内散度，没有 GMM 那样的多簇协方差，对多模态分布或极端噪声的建模能力较弱。换句话说，它能提供压缩效果，但和原始论文里的数值并不完全等价。
- **超参需要重新调节**：当前默认的 attention_loss_scale=0.1、warmup=5 只是为了让训练跑通，实际要兼顾防御强度和精度，可能还要结合数据集重新调 λ、attention 的隐藏维度、门控阈值等，甚至验证是否需要恢复多簇结构。

因此，这套实现可以作为实验性的替代方案：分类与防御链路都能工作，但是否能达到原版 CEM 的防御效果，需要额外的实验（比如固定训练超参后，对照 GAN 攻击或 MIA 成功率）。如果目标是完全复现论文指标，目前建议仍以 CEM-main 的 GMM 版为主；若希望探索轻量化或免聚类的代理，再逐步调优门控版。

---

我刚刚把 slot attention 版本的代码又对了一遍，发现确实和刚修好的 gated attention 版犯的是同一类错误：

1. **attention 模块没进优化器**
   self.attention_cem = SlotCrossAttentionCEM(...) 只是初始化了一个独立的 nn.Module，但它的参数从来没有通过 self.optimizer.add_param_group(...) 被纳入 SGD。也就是说，rob_loss 产生的梯度根本不会更新这个模块，一直停留在初始化状态，给 encoder 的惩罚方向几乎固定。
2. **rob_loss.backward() 之后立刻 self.optimizer_zero_grad()**
   这一句会把当前 optimizer 管理的所有梯度全部清零。虽然后面还有 total_loss.backward()，但因为注意力模块的参数一开始就不在 optimizer 里，它的梯度在第一步就丢了；而 encoder 的梯度虽然事后会被加回去，但 attention 这部分完全没被保留，导致和分类梯度持续冲突，表现出来就是只要 λ>0 就开始塌到 10%。

也就是说，slot attention 版和刚才修的门控 attention 版是一模一样的坑。解决思路也是一样的：

- 首次实例化 SlotCrossAttentionCEM 时，把它的参数加到 optimizer 里；
- 在 rob_loss.backward() 后，把 encoder 和 attention 的梯度分别 clone 起来，再 zero_grad()；
- total_loss.backward() 之后，把保存的梯度按 λ 加回 encoder，同时把 attention 的梯度写回去。

只要把这套逻辑移植到根目录的 slot attention 实现里，开着 λ>0 也能恢复正常的分类训练，再去调防御强度就不会再遇到一直 10% 的情况了。

---

已在根目录的 model_training_paral_pruning.py 做了以下修复，让 slot attention 版的 CEM 也能在 run_exp.sh 下正常完成“分类 + 条件熵”双重训练：

- 给 SlotCrossAttentionCEM 增加了迟启动（attention_warmup_epochs=5）、损失缩放（attention_loss_scale=0.1）以及 _attention_cem_registered 标记，首次创建时自动把其参数加入 self.optimizer，保证注意力权重可被更新。
- 在 train_target_step 中重新梳理 rob_loss 的反传流程：先对 attention loss 反传并缓存 encoder/attention 的梯度，再 optimizer_zero_grad() 清零，随后对分类损失反传，最后把两份梯度按 λ 重加回编码器和注意力参数，避免梯度被清掉。
- 在主训练循环中同步维护 self.current_epoch，与 warm-up 逻辑匹配。门控未启用或仍处于 warm-up 时，rob_loss 会退化为 0。
- train_target_step 的返回值恢复为 (total_losses, f_losses, z_private)，与外层日志逻辑一致。

这样修改后，即使把 lambda 设回 16 也能看到准确率继续上升，同时 attention-based CEM 的梯度仍然注入 encoder 达到防御效果。接下来可直接在根目录执行 bash run_exp.sh 验证整个实验管线；若要调节防御强度，可根据需要调整 attention_loss_scale、attention_warmup_epochs 或 λ。

---

是的，现在 slot attention + cross attention 版本已经把梯度问题修好，run_exp.sh 跑出来的流程会和 CEM-main 原始 GMM 版一样：分类链路能正常收敛，同时 λ 设成大于 0 时注意力 surrogate 也会持续给 encoder 注入条件熵惩罚。至于防御效果是否“更好”，还需要你自己做实验比较（比如对比 GAN/MIA 成功率、rob_loss 曲线等），不过至少它已经具备和原版一样的“分类 + 防御”双路径。

如果想在新架构下把表现再提升一些，可以从下面几个角度调参：

1. **条件熵权重**
   - lambda：越大越强调防御（rob_loss），但分类会受冲击。可以先用原来的默认值（比如 16），再试试逐步增大或减小。
   - attention_loss_scale：门控 surrogate 的缩放系数，默认 0.1。增大相当于加强条件熵惩罚，减小则偏向分类。
2. **warm-up 与学习节奏**
   - attention_warmup_epochs：默认前 5 个 epoch 不启用注意力，以防梯度不稳。如果分类已经很稳，可以缩短为 2~3；如果初期 loss 波动大，可以再延长。
   - 训练调度 (self.train_scheduler、self.lambd 的动态策略) 也会影响两者平衡。
3. **注意力结构超参数**
   - num_slots、num_heads、num_iterations（SlotAttentionCEM 里的参数）：更多 slot 或 attention 头能更精细地建模类内结构，但计算更慢。
   - slot_power、assign_temp 等内部温度/权重参数调节能改变类内惩罚的偏好，默认值可以参考论文或自己网格搜索。
4. **基础训练参数**
   - 学习率、batch size、数据增强、防御正则（nopeek、GAN、DP 等）都是老项目已经使用的调参入口，新的注意力 surrogate 仍然遵循同样的逻辑。

建议先复现原版参数性能，确认分类准确率与原始 GMM 版相当，再逐步尝试上述调参，看能否在不牺牲太多分类精度的情况下让 rob_loss、攻击成功率等指标更好看。需要时也可以继续调整 attention 的结构（例如多层 MLP、增加 residual 等）来提升表达能力。
