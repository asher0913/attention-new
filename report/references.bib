@misc{cem,
      title={Theoretical Insights in Model Inversion Robustness and Conditional Entropy Maximization for Collaborative Inference Systems}, 
      author={Song Xia and Yi Yu and Wenhan Yang and Meiwen Ding and Zhuo Chen and Ling-Yu Duan and Alex C. Kot and Xudong Jiang},
      year={2025},
      eprint={2503.00383},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2503.00383}, 
}
@article{vepakomma_splitnn,
  title={Split learning for health: Distributed deep learning without sharing raw patient data},
  author={Praneeth Vepakomma and Otkrist Gupta and Tristan Swedish and Ramesh Raskar},
  journal={ArXiv},
  year={2018},
  volume={abs/1812.00564},
  url={https://api.semanticscholar.org/CorpusID:54439509}
}

@misc{thapa_splitfed,
      title={SplitFed: When Federated Learning Meets Split Learning}, 
      author={Chandra Thapa and M. A. P. Chamikara and Seyit Camtepe and Lichao Sun},
      year={2022},
      eprint={2004.12088},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2004.12088}, 
}
@INPROCEEDINGS{shlezinger_edge_ensemble,
  author={Shlezinger, Nir and Farhan, Erez and Morgenstern, Hai and Eldar, Yonina C.},
  booktitle={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Collaborative Inference via Ensembles on the Edge}, 
  year={2021},
  volume={},
  number={},
  pages={8478-8482},
  keywords={Performance evaluation;Conferences;Neural networks;Collaboration;Signal processing;Delays;Artificial intelligence;Edge computing;deep ensembles;neural networks},
  doi={10.1109/ICASSP39728.2021.9414740}}

@inproceedings{fredrikson_mi,
author = {Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
title = {Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures},
year = {2015},
isbn = {9781450338325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810103.2813677},
doi = {10.1145/2810103.2813677},
abstract = {Machine-learning (ML) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses, and facial recognition. In a model inversion attack, recently introduced in a case study of linear classifiers in personalized medicine by Fredrikson et al., adversarial access to an ML model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs, however, is unknown. We develop a new class of model inversion attack that exploits confidence values revealed along with predictions. Our new attacks are applicable in a variety of settings, and we explore two in depth: decision trees for lifestyle surveys as used on machine-learning-as-a-service systems and neural networks for facial recognition. In both cases confidence values are revealed to those with the ability to make prediction queries to models. We experimentally show attacks that are able to estimate whether a respondent in a lifestyle survey admitted to cheating on their significant other and, in the other context, show how to recover recognizable images of people's faces given only their name and access to the ML model. We also initiate experimental exploration of natural countermeasures, investigating a privacy-aware decision tree training algorithm that is a simple variant of CART learning, as well as revealing only rounded confidence values. The lesson that emerges is that one can avoid these kinds of MI attacks with negligible degradation to utility.},
booktitle = {Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},
pages = {1322–1333},
numpages = {12},
keywords = {attacks, machine learning, privacy},
location = {Denver, Colorado, USA},
series = {CCS '15}
}

@inproceedings{hitaj_gan_mi,
author = {Hitaj, Briland and Ateniese, Giuseppe and Perez-Cruz, Fernando},
title = {Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning},
year = {2017},
isbn = {9781450349468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3133956.3134012},
doi = {10.1145/3133956.3134012},
abstract = {Deep Learning has recently become hugely popular in machine learning for its ability to solve end-to-end learning systems, in which the features and the classifiers are learned simultaneously, providing significant improvements in classification accuracy in the presence of highly-structured and large databases.Its success is due to a combination of recent algorithmic breakthroughs, increasingly powerful computers, and access to significant amounts of data.Researchers have also considered privacy implications of deep learning. Models are typically trained in a centralized manner with all the data being processed by the same training algorithm. If the data is a collection of users' private data, including habits, personal pictures, geographical positions, interests, and more, the centralized server will have access to sensitive information that could potentially be mishandled. To tackle this problem, collaborative deep learning models have recently been proposed where parties locally train their deep learning structures and only share a subset of the parameters in the attempt to keep their respective training sets private. Parameters can also be obfuscated via differential privacy (DP) to make information extraction even more challenging, as proposed by Shokri and Shmatikov at CCS'15.Unfortunately, we show that any privacy-preserving collaborative deep learning is susceptible to a powerful attack that we devise in this paper. In particular, we show that a distributed, federated, or decentralized deep learning approach is fundamentally broken and does not protect the training sets of honest participants. The attack we developed exploits the real-time nature of the learning process that allows the adversary to train a Generative Adversarial Network (GAN) that generates prototypical samples of the targeted training set that was meant to be private (the samples generated by the GAN are intended to come from the same distribution as the training data). Interestingly, we show that record-level differential privacy applied to the shared parameters of the model, as suggested in previous work, is ineffective (i.e., record-level DP is not designed to address our attack).},
booktitle = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
pages = {603–618},
numpages = {16},
keywords = {collaborative learning, deep learning, privacy, security},
location = {Dallas, Texas, USA},
series = {CCS '17}
}

@inproceedings{noise_arl,
author = {Jeong, Jonghu and Cho, Minyong and Benz, Philipp and Kim, Tae-hoon},
title = {Noisy adversarial representation learning for effective and efficient image obfuscation},
year = {2023},
publisher = {JMLR.org},
abstract = {Recent real-world applications of deep learning have led to the development of machine learning as a service (MLaaS). However, the scenario of client-server inference presents privacy concerns, where the server processes raw data sent from the user's client device. One solution to this issue is to provide an obfuscator function to the client device using Adversarial Representation Learning (ARL). Prior works have primarily focused on the privacy-utility trade-off while overlooking the computational cost and memory burden on the client side. In this paper, we propose an effective and efficient ARL method that incorporates feature noise into the ARL pipeline. We evaluated our approach on various datasets, comparing it with state-of-the-art ARL techniques. Our experimental results indicate that our method achieves better accuracy, lower computation and memory overheads, and improved resistance to information leakage and reconstruction attacks.},
booktitle = {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
articleno = {90},
numpages = {10},
location = {Pittsburgh, PA, USA},
series = {UAI '23}
}

@INPROCEEDINGS {patrol,
author = { Ding, Shiwei and Zhang, Lan and Pan, Miao and Yuan, Xiaoyong },
booktitle = { 2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) },
title = {{ PATROL: Privacy-Oriented Pruning for Collaborative Inference Against Model Inversion Attacks }},
year = {2024},
volume = {},
ISSN = {},
pages = {4704-4713},
abstract = { Collaborative inference has been a promising solution to enable resource-constrained edge devices to perform inference using state-of-the-art deep neural networks (DNNs). In collaborative inference, the edge device first feeds the input to a partial DNN locally and then uploads the intermediate result to the cloud to complete the inference. However, recent research indicates model inversion attacks (MIAs) can reconstruct input data from intermediate results, posing serious privacy concerns for collaborative inference. Existing perturbation and cryptography techniques are inefficient and unreliable in defending against MIAs while performing accurate inference. This paper provides a viable solution, named PATROL, which develops privacy-oriented pruning to balance privacy, efficiency, and utility of collaborative inference. PATROL takes advantage of the fact that later layers in a DNN can extract more task-specific features. Given limited local resources for collaborative inference, PATROL intends to deploy more layers at the edge based on pruning techniques to enforce task-specific features for inference and reduce task-irrelevant but sensitive features for privacy preservation. To achieve privacy-oriented pruning, PATROL introduces two key components: Lipschitz regularization and adversarial reconstruction training, which increase the reconstruction errors by reducing the stability of MIAs and enhance the target inference model by adversarial training, respectively. On a real-world collaborative inference task, vehicle re-identification, we demonstrate the superior performance of PATROL in terms of against MIAs. },
keywords = {Training;Performance evaluation;Privacy;Perturbation methods;Collaboration;Artificial neural networks;Feature extraction},
doi = {10.1109/WACV57701.2024.00465},
url = {https://doi.ieeecomputersociety.org/10.1109/WACV57701.2024.00465},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Jan}


@INPROCEEDINGS {distcorr,
author = { Vepakomma, Praneeth and Singh, Abhishek and Gupta, Otkrist and Raskar, Ramesh },
booktitle = { 2020 International Conference on Data Mining Workshops (ICDMW) },
title = {{ NoPeek: Information leakage reduction to share activations in distributed deep learning }},
year = {2020},
volume = {},
ISSN = {},
pages = {933-942},
abstract = { For distributed machine learning with sensitive data, we demonstrate how minimizing distance correlation between raw data and intermediary representations reduces leakage of sensitive raw data patterns across client communications while maintaining model accuracy. Leakage (measured using distance correlation between input and intermediate representations) is the risk associated with the invertibility of raw data from intermediary representations. This can prevent client entities that hold sensitive data from using distributed deep learning services. We demonstrate that our method is resilient to such reconstruction attacks and is based on reduction of distance correlation between raw data and learned representations during training and inference with image datasets. We prevent such reconstruction of raw data while maintaining information required to sustain good classification accuracies. },
keywords = {Deep learning;Training;Privacy;Correlation;Distributed databases;Image reconstruction;Visual perception},
doi = {10.1109/ICDMW51313.2020.00134},
url = {https://doi.ieeecomputersociety.org/10.1109/ICDMW51313.2020.00134},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Nov}


@article{srivastava_dropout,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
title = {Dropout: a simple way to prevent neural networks from overfitting},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1929–1958},
numpages = {30},
keywords = {deep learning, model combination, neural networks, regularization}
}

@inproceedings{slot_attention,
author = {Locatello, Francesco and Weissenborn, Dirk and Unterthiner, Thomas and Mahendran, Aravindh and Heigold, Georg and Uszkoreit, Jakob and Dosovitskiy, Alexey and Kipf, Thomas},
title = {Object-centric learning with slot attention},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Learning object-centric representations of complex scenes is a promising step towards enabling efficient abstract reasoning from low-level perceptual features. Yet, most deep learning approaches learn distributed representations that do not capture the compositional properties of natural scenes. In this paper, we present the Slot Attention module, an architectural component that interfaces with perceptual representations such as the output of a convolutional neural network and produces a set of task-dependent abstract representations which we call slots. These slots are exchangeable and can bind to any object in the input by specializing through a competitive procedure over multiple rounds of attention. We empirically demonstrate that Slot Attention can extract object-centric representations that enable generalization to unseen compositions when trained on unsupervised object discovery and supervised property prediction tasks.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {967},
numpages = {14},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

flamingo
@inproceedings{flamingo,
author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millicah, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
title = {Flamingo: a visual language model for few-shot learning},
year = {2022},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {1723},
numpages = {21},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

% -------------------- added for interim report --------------------
@INPROCEEDINGS{nasr_whitebox,
  author={Nasr, Milad and Shokri, Reza and Houmansadr, Amir},
  booktitle={2019 IEEE Symposium on Security and Privacy (SP)}, 
  title={Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning}, 
  year={2019},
  pages={739-753},
  doi={10.1109/SP.2019.00065}
}

@misc{zhu_dlg,
  title={Deep Leakage from Gradients}, 
  author={Ligeng Zhu and Zhijian Liu and Song Han},
  year={2019},
  eprint={1906.08935},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@inproceedings{carlini_diffusion,
  author = {Carlini, Nicholas and Hayes, Jamie and Nasr, Milad and Jagielski, Matthew and Sehwag, Vikash and Tramer, Florian and Balle, Borja and Ippolito, Daphne and Wallace, Eric},
  title = {Extracting Training Data from Diffusion Models},
  booktitle = {32nd USENIX Security Symposium (USENIX Security 23)},
  year = {2023},
  pages = {5253--5270},
  url = {https://www.usenix.org/conference/usenixsecurity23/presentation/carlini-extracting},
  publisher = {USENIX Association}
}
