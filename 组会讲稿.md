在这一轮项目复盘里，我计划直接把我对注意力替换方案的完整思考过程和代码走向整理成讲稿，让我们明天在组会上能够顺着这份文稿一点一点推进。我不会再回过头去讲原来高斯混合模型是怎样一步步构建条件熵代理的历史背景，而是聚焦在我们现在真正落地的注意力模块，说明它是怎样在代码层面接管原来 GMM 执行的位置，怎样在训练过程中发挥作用，又是怎样因为门控和稳定化技巧而不断演化，最后再说清楚它当下给我们带来的成果、问题、挑战。我会按照我对导师汇报的语气和逻辑，把细节尽可能完整地展开，所以内容会比较长，但这也是为了确保组会期间我们可以不间断地讨论。  

首先我想从注意力模块的诞生开始，也就是我们在 `model_training_paral_pruning.py:40` 一直延展到 `model_training_paral_pruning.py:208` 这一段代码里，完整定义的 SlotAttention、CrossAttention 和 SlotCrossAttentionCEM 三个类。这三者构成了我们目前的条件熵代理。在代码上我们将 SlotAttention 放在最前面，是因为我们希望先把同一类别的样本，通过 slot 的形式归纳成几个代表性的“注意力槽位”。在当前实现里，我们输入到 SlotAttention 的张量在 batch 维度之外只有一维特征向量，所以 `inputs` 在 `forward` 里表现为 `[B, N, D]` 的形状，这里的 N 在初版里还是等于一，但这样规划的好处是我们未来想要把特征分解为多个 token 的时候，就不需要重写接口了。SlotAttention 的流程比较遵循原始论文，先通过 `self.norm_inputs` 把输入做层归一化，再透过 `self.to_k` 和 `self.to_v` 映射出键和值，随后用 `self.slot_mu` 和 `self.slot_log_sigma` 按高斯方式初始化 slots。在迭代循环中，我们看到 `slots_prev = slots`，然后 `slots_norm = self.norm_slots(slots)` 把槽位再次归一化，再用 `self.to_q` 投影出查询向量，之后通过 `torch.einsum('bnd,bsd->bns', k, q)` 计算注意力 logits。在这一行之后我们刻意加入了 `attn_logits = attn_logits / (self.slot_dim ** 0.25)` 的温度缩放，是因为我们在调试中发现高维特征下 logits 很容易爆炸，这个指数缩放相当于在原本的 `self.scale` 之外再降一次温。随后 `attn = torch.softmax(attn_logits, dim=-1)`，并且加上 `self.eps` 避免出现全零的情形，然后用 `attn_sum = attn.sum(dim=1, keepdim=True)` 再次归一化，就能得到比较稳定的权重分布。更新步骤里我们用 `torch.einsum('bns,bnd->bsd', attn, v)` 聚合 value，之后将更新写入 `self.gru`，实现 slot 内的循环优化，再走一次 MLP 残差。这一段流程和我们后面 CrossAttention 的对接非常关键，因为 SlotAttention 的输出 shape 是 `[B, self.num_slots, self.slot_dim]`，在我们当前的训练逻辑里，就是给 CrossAttention 提供键和值的集合。  

接下来是 CrossAttention，也就是我们在 `model_training_paral_pruning.py:94` 到 `model_training_paral_pruning.py:136` 之间定义的类。这个类在设计上其实并不是普通 Transformer 的 cross-attention，而是参考 Flamingo 论文里提出的 Gated XAttn-Dense 结构。首先，构造函数里我们要求 `feature_dim % num_heads == 0`，并根据 `feature_dim` 计算 `self.head_dim`，这是保证 multi-head 分块的基础。在 `__init__` 中我们依次建立三套 LayerNorm：`self.ln_q`、`self.ln_kv`、`self.ln_ff`，用于对 query、key-value 以及后续的前馈网络做归一化。线性投影部分使用 `self.to_q`、`self.to_k`、`self.to_v` 三个无偏置的全连接层，保留了常见的注意力结构。在注意力输出后，我们还使用了一个常规的全连接 `self.proj` 配合 `self.proj_drop` 做投影和 dropout。真正的改动点在两个参数：`self.alpha_xattn` 和 `self.alpha_ffn`。它们是需要训练的标量，并且初始化为 `torch.tensor(0.1)`。我们在 `forward` 里看到 `y = query_features + torch.tanh(self.alpha_xattn) * out`，这段代码的意思是把注意力输出通过 `tanh` 的函数压缩到 [-1, 1] 之后，再乘以 alpha 写回到原始特征上；同样地，`y_ff = self.ffn(self.ln_ff(y))` 之后也通过 `torch.tanh(self.alpha_ffn)` 做门控。这种设计的初衷是我们不再像普通残差网络那样直接把新信息完全叠加到旧特征上，而是由网络自己决定注意力输出和 FFN 输出的“参与度”。在训练初期，这两个门基本上都是 0.1 的 `tanh`，也就是约 0.1 的比例，意味着注意力部分只会以一个非常谨慎的幅度影响原特征；随着训练推进，alpha 可以逐渐学到更高或更低的值，模型便能自动调节对注意力的依赖度。这个门控在我们后续的实验中起到了非常重要的作用，因为没有门控时，注意力直接写回的强度太大，很快就会把 encoder 输出扰乱得难以收敛，而门控给了我们一个柔和的过渡。  

当 SlotAttention 和 CrossAttention 都准备好之后，我们在 `model_training_paral_pruning.py:138` 到 `model_training_paral_pruning.py:209` 之间定义了 `SlotCrossAttentionCEM`，这就是新的条件熵代理本体。在这个类里，`__init__` 做的事情有三件：第一，保存 `feature_dim`、`num_slots`、`num_heads` 等超参数，并将 `self.slot_attention` 和 `self.cross_attention` 初始化好；第二，设置一些用于数值稳定的参数，比如 `self.eps_var`、`self.var_threshold` 和 `self.reg_strength`；第三，原地存下 `var_threshold` 和正则强度的初值，这样 forward 的时候就可以直接使用。这里的 `feature_dim` 是传入的扁平化特征长度，我们目前在客户端训练里直接把 `z_private` reshape 成 `[B, D]`，所以 `feature_dim` 就是 `C×H×W`，这一点后面会成为我们遇到的瓶颈。  

`SlotCrossAttentionCEM.forward` 的核心从 `model_training_paral_pruning.py:149` 一直写到 `model_training_paral_pruning.py:209`。我们先定义了 `total_logvar`、`total_mse` 和 `total_weight` 作为累积变量，而且它们都在 `device` 上运行，避免从 GPU 搬回 CPU。在循环中，我们对 `unique_labels` 里的每一个类别执行一次处理：首先取出 `features[mask]`，如果某类别样本少于 2 个，就直接跳过，因为无法算方差。接着，`class_feats_norm = F.layer_norm(class_feats, class_feats.shape[1:])` 对当前类别所有样本做 layer norm，这一步的目的是让不同类别之间的尺度差异减小，避免 slot attention 只关注数值范围大的样本。随后 `tokens = class_feats_norm.unsqueeze(0)`，把这些特征整理成 `[1, M, D]` 的形式送入 SlotAttention，输出 `slots`，再做 `enhanced = self.cross_attention(class_feats_norm, slots)`，获得注意力增强后的特征。  

增强后的特征我们会先取均值 `mean_c = enhanced.mean(dim=0)`，再用 `F.mse_loss(enhanced, mean_c.expand_as(enhanced))` 得到类内 MSE 作为监控指标。然后 `var_c = enhanced.var(dim=0, unbiased=True)`，保证采用无偏估计，并且用 `torch.clamp(var_c, min=self.eps_var)` 防止出现负方差或零方差。之后 `logvar = torch.log(var_c + gamma)`，这里 `gamma = 1e-6` 是防止 `log(0)` 的补偿项。阈值部分 `logvar_thr = torch.tensor(max(self.var_threshold * (self.reg_strength ** 2), 1e-8) + gamma, device=device, dtype=dtype)`，这段逻辑表面上看是把 `var_threshold` 和 `regularization_strength` 结合起来；我们默认的 `var_threshold` 是 0.1，如果 `regularization_strength` 也比较小，乘积就会趋向于零，所以才会有 `max(..., 1e-8)` 的保护。`ce_sur = F.relu(logvar - log_threshold)` 的作用是：当 `logvar` 小于阈值时条件熵认为达标，结果为 0；超过阈值时，就把超出的部分当成 penalty。最后 `logvar_c = ce_sur.mean()` 得到一个标量，结合 `mse_c` 分别累加到 `total_logvar` 和 `total_mse`，并用类别的样本占比去加权。整个循环结束后，如果 `total_weight > 0`，就返回 `rob_loss = total_logvar / total_weight` 和 `intra_mse = total_mse / total_weight`；如果一个类别都没处理，我们就返回全零张量。这个 Forward 和 GMM 版的 `compute_class_means` 行为一致，都会返回两个指标供后续使用。  

上面说的是模块本身，接下来要讲的是我们如何在训练流程中使用它。请看 `model_training_paral_pruning.py:504-506`，在 `MIA_train.__init__` 里，我们直接把 `self.use_attention_cem` 固定为 True，并且把 `self.attention_cem` 初始化为 None，这是为了延迟构造。当训练开始时，在 `train_target_step` 中如果我们判断 `self.lambd > 0` 且不是随机初始化中心的阶段，就会进入注意力分支。在 `model_training_paral_pruning.py:1223` 以后，可以看到我们首先把 `z_private` 转成扁平化特征 `feats_flat`，同时也做了 `torch.isnan` 和 `torch.isinf` 检查，避免输入本身含有非法值。如果是第一次调用，这里 `self.attention_cem` 还是 None，我们就在 `model_training_paral_pruning.py:1237` 处把它初始化为 `SlotCrossAttentionCEM(...)`，并且用 `.to(feats_flat.device)` 放到 GPU 上。之后 `rob_loss, intra_class_mse = self.attention_cem(feats_flat, label_private, unique_labels)`，就完成了整套注意力 surrogate 的计算。  

得到 `rob_loss` 后，我们保留了原来 GMM 时代的梯度注入方式，也就是说先执行 `rob_loss.backward(retain_graph=True)`，拿到编码器部分的梯度拷贝，随后立即执行 `self.optimizer_zero_grad()` 把梯度清空，再去算交叉熵的梯度。之所以这么做，是为了保证鲁棒正则化对 encoder 的影响和分类损失的影响能够按照我们想要的比例混合。在 `model_training_paral_pruning.py:1405` 到 `model_training_paral_pruning.py:1415` 的这一段里，我们可以看到对每个编码器参数 `param`，我们先检查之前保存的 `encoder_gradients[name]` 是否存在，再根据学习率调整梯度。如果 `self.load_from_checkpoint` 为 True，就直接按 `self.lambd` 叠加；否则根据当前学习率的大小，选用两个不同的缩放方式：当学习率小于 4.1e-4 时，使用 `param.grad += self.lambd * encoder_gradients[name]`；否则使用 `param.grad += self.lambd * encoder_gradients[name] * (0.001 / self.train_scheduler.get_last_lr()[0])`，目的是在学习率大的时候缩小鲁棒梯度，以免训练初期因为鲁棒正则过强导致分类迟迟学不上去。  

训练流程整合完后还有一些配套的工程修复需要一并提及，因为它们保证了注意力替换之后不会再被历史问题困扰。最显眼的就是在 `model_training_paral_pruning.py:1422-1426` 对返回值的修正。之前由于我们返回的是 `(intra_class_mse, f_losses, z_private)`，导致上层函数把 class mse 当成总损失，Prec@1 永远在 10% 左右。现在改为 `return total_losses, f_losses, z_private`，日志已经恢复正常。其次是 `main_MIA.py:4-6` 和 `main_test_MIA.py:4-13` 把 matplotlib 切换到了 `Agg` 后端，这样在服务器上看日志不会再遇到 Qt 错误。另外我们还新增了 `generate_test_data.py` 脚本，解决 `test_cifar10_image.pt` 缺失导致 demo 跑不起来的问题。  

在这一切都搭建起来之后，我们观察到的现状是，注意力 surrogate 的确成功地替换了 GMM：训练流程不再依赖 CPU 的 GMM 拟合，也无需维护 PCA 模块；所有条件熵的估计都在 GPU 内部完成，并且自带可学习的参数，可以随任务自适应。在实验中，我们还得到了一个额外的好处，就是鲁棒性显著提升——我们在多次 runs 中都看到防御性的损失值很理想，这意味着注意力代理确实把类内方差压得很厉害。  

然而，这样的压制性也带来了明显的副作用。首先我们必须正视参数规模的问题。当前 `z_private` 在大多数模型中都是 `[B, C, H, W]`，而我们直接用 `view(B, -1)` 把它 flatten 成一个 D 维向量，这个 D 动辄就是几千、上万。例如 VGG 的中间层有可能是 512 通道乘上 7×7，flatten 就是 25088 维。SlotAttention 和 CrossAttention 的线性层需要处理的矩阵就是 `25088 × 25088` 级别的，参数量本身就是几百兆，而且 forward 过程中伴随大量矩阵乘法，其梯度也极其不稳定。这也是为什么在训练后期分类准确率会一直卡在某个很低的水平，往往只能偶尔突破 20% 左右，很难达到 GMM 写法中常见的 60% 或更高。这一点在日志里表现得非常鲜明，一旦 `rob_loss` 太大，梯度再经 `self.lambd` 缩放后加入 encoder，就会把 encoder 原本用于分类的梯度掩盖掉。  

第二个关键问题是注意力模块未纳入优化器。我们在 `train_target_step` 中只是调用 `self.attention_cem` 的 forward，没有给它额外的 optimizer，也没有在我们的 `optimizer_zero_grad` 里清理它的梯度。理论上 PyTorch 会因为没有触碰它的参数梯度而让它保持 `requires_grad=True` 的状态，但实际上的问题是：这部分参数处于默认随机初始化，未经过任何训练，却被放在敏感的正则任务里。由于我们没有像 encoder 那样保存它的梯度拷贝，它实际上的训练效果非常有限。这可以解释为什么我们即使跑了几十个 epoch，注意力模块似乎也没有学到更好的方差代理：它的参数几乎停留在初始化状态，只是通过 LayerNorm 和 softmax 的结构把样本均匀分配给 slots。这一点在长时间训练后的日志中仍能看到前后跑出来的 `rob_loss` 曲线几乎重叠。  

第三个问题出现在阈值的设计上。我们把 `logvar_thr` 设置为 `log(self.var_threshold * (self.reg_strength ** 2) + gamma)`。如果我们在命令行里没有显式赋值 `regularization_strength`，它默认就是 0，那么 `self.var_threshold * (self.reg_strength ** 2)` 就是 0，最终阈值就是 `log(1e-6 + 1e-6)`，约等于 `log(2e-6)`，也就是一个非常大的负数。在这种情况下，只要方差略大于 0，`logvar` 通常都会比这个阈值大很多，于是 `F.relu(logvar - log_threshold)` 总是一个正值，结果就是每个类别都被认为“方差太大”。我们虽然在 encoder 梯度融合时有一个学习率依赖的缩放，但整体上这个罚项还是偏重。换句话说，只要 `self.lambd` 合理，我们就持续对 encoder 追加大量的负梯度，让特征全部朝着“类内分散极小”的方向发展。对抗攻击来说这当然是好事，但分类自然能力被牺牲掉了。  

第四个问题也是我们在下一阶段要重点突破的：我们目前的 SlotAttention 实际上不是真正意义上的“把特征分解成多个 token”。从 `class_feats_norm.unsqueeze(0)` 可以看到，我们只是把整个 flatten 的特征当成一个 token，然后用 slot 数量为 8 的 SlotAttention 去拟合一个完全冗余的表示。SlotAttention 原本在视觉任务里发挥作用是因为它能从空间维度抽象出“object”，也就是把 `H×W` 的像素 token 打包成一些 slots，再用于后续 cross attention。如果我们继续用单 token 的方式，就无法发挥 slot 的作用，反倒是把参数量放大了。这也说明我们下一步必须要么引入一个降维层（例如 1×1 卷积或线性层，把 25088 压缩到 128），要么在 flatten 之前把 feature map 重新排列成 `[B, H×W, C]` 的形式，让每个位置成为一个 token。  

除了注意力模块本身，我们还要提到这套系统在工程上做的修复。比如 `model_training_paral_pruning.py:777` 到 `model_training_paral_pruning.py:777` 的 `optimizer_zero_grad` 把所有本地优化器都清空，避免遗留梯度叠加；`model_training_GMM.py:1292` 把 t-SNE 的 `n_iter` 改成 `max_iter`，兼容新版本 `sklearn`；`utils.py:142` 在 logger 获取时加入 `if logger.handlers` 的判空，避免重复添加 handler。这些看似琐碎的改动保证了我们在注意力替换的主线之外，原来的 bug 不会再干扰我们。  

面对这些问题，我们给自己规划了后续的调整方向。首先就是控制特征尺寸。我在本地实验里尝试过通过 `torch.nn.Conv2d` 加上一层 `torch.nn.Flatten` 的方式，把 `z_private` 先压缩到 `B×128`，这样 `SlotCrossAttentionCEM` 的线性层规模大幅降低，可以显著缓解注意力模块的梯度震荡。当然，这一步会改变原始的特征分布，需要验证对攻击鲁棒性的影响，所以我们会在组会上讨论是否先试 `Linear` 的简单方案，再考虑更复杂的 token 化。  

其次，我们需要决定注意力模块要不要主动训练。如果我们认为注意力是一个真正的学习代理，那么我们就应该给它单独的 optimizer，例如 `torch.optim.Adam(self.attention_cem.parameters(), lr=1e-4)`，并在 `optimizer_zero_grad` 内把这个优化器也纳入清空列表。反之，如果我们当前更担心准确率崩掉，可以先把 `self.attention_cem` 冻结，设定 `param.requires_grad = False`，让它作为一个固定的统计层，先观察分类性能能否恢复到原来的水平。  

第三，我们要重新思考阈值。可以考虑把 `logvar_thr` 改成一个固定常数，比如 `log(0.05)`，或者使用一个随 epoch 增加的 schedule，让模型先学出分类能力，后期再逐步加强鲁棒性。更进一步，我们也可以借鉴信息瓶颈理论，直接计算 `log` 的差值并加上柔性系数，而不是简单地用 `relu`。  

最后，保留 GMM 作为对照实验也是必须的。我们现在完全切换到注意力后，缺少一个直接可比的 baseline，如果导师希望看到注意力相较于 GMM 的优势和劣势，我们需要在代码中保留一个可选分支，运行 `self.use_attention_cem = False` 的版本，比较两个方案在相同训练时长下的准确率、攻击成功率、训练耗时。  

我把这些内容整理成这篇完整讲稿，目的是希望明天的组会上大家可以顺着这条线，把“注意力替换 GMM 之后发生了什么”讲得明明白白。逻辑上先讲模块定义，再讲训练中如何调用，再谈门控和稳定化的细节，最后说实验中遇到的困难和我们的下一步打算。整套系统已经跑通，但要达到理论上兼顾准确率与鲁棒性的目标还需要大量调参，我们也已经锁定了四个首要问题：参数规模、未被优化的注意力、阈值的设定以及 token 化的缺失。只要把这些问题拆开来逐一解决，我相信这套注意力代理体系是有潜力超过原始 GMM 的，至少在防御能力这一块，它已经表现出了惊人的效果。明天我们就按照这份稿子的顺序交流，争取在会议上获得导师的建议，为下一阶段的修改打好基础。  
