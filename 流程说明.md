# CEM 流程文字说明（原始 GMM vs. 注意力替换版）

## 一、原始 CEM-main（GMM 条件熵代理）流程
1. **输入与客户端前向**  
   - `train_target_step` 从 `model_training_paral_pruning.py:1018` 取得 `(x_private, label_private)`。  
   - 调用客户端网络 `self.f(x_private)`（`model_training_paral_pruning.py:1050`），得到中间特征 `z_private`。

2. **防御策略注入（可选）**  
   - 根据配置依次检查局部差分隐私、dropout、top-k 裁剪、GAN 噪声等选项（`model_training_paral_pruning.py:1085-1188`），对 `z_private` 进行扰动或保持原状。

3. **条件熵 surrogate 计算**  
   - 若 `random_ini_centers` 为真或 `λ≤0`，跳过鲁棒性计算，`rob_loss` 记为 0。  
   - 否则调用 `compute_class_means`（`model_training_paral_pruning.py:885-1044`）：  
     1. 将 `z_private` 重新排序并展平；  
     2. 按标签检索已有的 `centroids_list`、`weights_list`、`cluster_variances_list`，必要时运行 PCA/GMM 拟合；  
     3. 计算每个样本到最近簇中心的距离与方差；  
     4. 将各簇方差按权重组合，得到类内条件熵 surrogate；  
     5. 返回 `rob_loss` 与 `intra_class_mse`。

4. **噪声正则（可选）与分类前向**  
   - 若启用了 Gaussian 正则，在 `model_training_paral_pruning.py:1258-1269` 给 `z_private` 注入高斯噪声。  
   - 将激活送入服务端尾部网络与分类器，计算交叉熵损失（`model_training_paral_pruning.py:1308-1333`）。

5. **梯度回传与参数更新**  
   - 先对 `rob_loss` 执行 `backward`，缓存编码器梯度，再清零优化器；  
   - 随后对总损失（交叉熵 + 其他正则）求导，并将前面缓存的鲁棒梯度按学习率缩放后加回编码器参数（`model_training_paral_pruning.py:1353-1417`）；  
   - 最终调用 `optimizer_step` 完成客户端与服务器端参数更新。

## 二、注意力版（Slot/Cross Attention 条件熵代理）流程
1. **输入与客户端前向**  
   - `train_target_step` 从 `model_training_paral_pruning.py:1202` 抓取批次数据，`self.f` 输出 `z_private`（`model_training_paral_pruning.py:1214`）。

2. **防御策略注入（保持与原版一致）**  
   - 局部差分隐私、dropout、top-k 裁剪、GAN 噪声等仍在 `model_training_paral_pruning.py:1274-1384` 中执行，与原版逻辑相同。

3. **注意力条件熵 surrogate**  
   - 若 `random_ini_centers` 为真或 `λ≤0`，`rob_loss` 置零。  
   - 否则执行以下步骤（`model_training_paral_pruning.py:1224-1255`）：  
     1. 将 `z_private` 展平为 `[B, D]`；  
     2. 若 `self.attention_cem` 尚未实例化，则以 `D` 为输入维度创建 `SlotCrossAttentionCEM`；  
     3. 对每个类别调用 `SlotCrossAttentionCEM.forward`：  
        - 同类样本先经 SlotAttention 聚合，得到若干 slot；  
        - 再以 slot 为键值，通过 CrossAttention（带门控残差）强化样本特征；  
        - 分别计算类内均方误差和方差，与阈值比较生成条件熵 surrogate；  
     4. 汇总各类别的结果，得到 `rob_loss` 与 `intra_class_mse`。

4. **噪声正则（可选）与分类前向**  
   - 若启用 Gaussian 正则，在 `model_training_paral_pruning.py:1264-1272` 添加噪声；  
   - 将激活传入服务器端与分类器，计算交叉熵损失（`model_training_paral_pruning.py:1320-1352`）。

5. **梯度回传与参数更新**  
   - 先对 `rob_loss` 求导并提取编码器梯度副本（`model_training_paral_pruning.py:1387-1400`）；  
   - 清空梯度后，对总损失反向传播，并将鲁棒梯度按学习率与 `λ` 调整后加回编码器（`model_training_paral_pruning.py:1403-1417`）；  
   - 调用 `optimizer_step`、`scheduler_step` 更新参数。

## 三、关键差异总结
- **条件熵代理**：原版依赖 GMM/PCA 等统计工具，注意力版完全在 GPU 中通过 Slot/Cross Attention 计算，可学习且无需额外拟合。  
- **防御模块**：两者使用同一套激活防御策略，差异集中在 surrogate 的实现。  
- **梯度注入方式**：两者均先单独回传鲁棒性损失，再与交叉熵梯度合并，只是注意力版的 `rob_loss` 来自神经网络。  
- **调优重点**：注意力版需要关注参数规模、门控系数、阈值设定；原版需关注 GMM 拟合的稳定性与效率。
